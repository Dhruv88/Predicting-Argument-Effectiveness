{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12e0435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:53.417542Z",
     "iopub.status.busy": "2022-11-13T09:55:53.417145Z",
     "iopub.status.idle": "2022-11-13T09:55:55.033716Z",
     "shell.execute_reply": "2022-11-13T09:55:55.032689Z"
    },
    "id": "kb1dt1Bjt9NA",
    "outputId": "083d6bb7-751f-47d3-9cc1-4f24d0a81a70",
    "papermill": {
     "duration": 1.632773,
     "end_time": "2022-11-13T09:55:55.036507",
     "exception": false,
     "start_time": "2022-11-13T09:55:53.403734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea62ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.054468Z",
     "iopub.status.busy": "2022-11-13T09:55:55.053488Z",
     "iopub.status.idle": "2022-11-13T09:55:55.058571Z",
     "shell.execute_reply": "2022-11-13T09:55:55.057706Z"
    },
    "id": "FDp9IMcK17KC",
    "outputId": "f7c2dbcd-67bc-479b-f600-71088225993f",
    "papermill": {
     "duration": 0.016077,
     "end_time": "2022-11-13T09:55:55.060704",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.044627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\"I ate dinner.\", \"We had a three-course meal.\", \"Brad came to dinner with us.\", \"He loves fish tacos.\",\"In the end, we all felt like we ate too much.\",\"We all agreed; it was a magnificent evening.\"]\n",
    "\n",
    "# Tokenization of each document\n",
    "# tokenized_sent = []\n",
    "# for s in sentences:\n",
    "#     tokenized_sent.append(word_tokenize(s.lower()))\n",
    "# print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ad7ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.077982Z",
     "iopub.status.busy": "2022-11-13T09:55:55.077185Z",
     "iopub.status.idle": "2022-11-13T09:55:55.082909Z",
     "shell.execute_reply": "2022-11-13T09:55:55.081831Z"
    },
    "id": "C1zmfAan2KT-",
    "papermill": {
     "duration": 0.01651,
     "end_time": "2022-11-13T09:55:55.084962",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.068452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b54084f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.101247Z",
     "iopub.status.busy": "2022-11-13T09:55:55.100920Z",
     "iopub.status.idle": "2022-11-13T09:55:55.104921Z",
     "shell.execute_reply": "2022-11-13T09:55:55.104002Z"
    },
    "id": "BDXhq1ac2Bq8",
    "outputId": "5c56ea5f-a2f5-4293-cde3-b292892eb3c8",
    "papermill": {
     "duration": 0.014235,
     "end_time": "2022-11-13T09:55:55.106944",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.092709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# import gensim\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "# tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09606fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.123261Z",
     "iopub.status.busy": "2022-11-13T09:55:55.122583Z",
     "iopub.status.idle": "2022-11-13T09:55:55.126915Z",
     "shell.execute_reply": "2022-11-13T09:55:55.125981Z"
    },
    "id": "1DMAZaNc2-hr",
    "outputId": "ee73e5e4-c815-4376-a1fe-ace3686e0a75",
    "papermill": {
     "duration": 0.014545,
     "end_time": "2022-11-13T09:55:55.128957",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.114412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train doc2vec model\n",
    "# model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "# '''\n",
    "# vector_size = Dimensionality of the feature vectors.\n",
    "# window = The maximum distance between the current and predicted word within a sentence.\n",
    "# min_count = Ignores all words with total frequency lower than this.\n",
    "# alpha = The initial learning rate.\n",
    "# '''\n",
    "\n",
    "## Print model vocabulary\n",
    "# model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b090502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.146885Z",
     "iopub.status.busy": "2022-11-13T09:55:55.146356Z",
     "iopub.status.idle": "2022-11-13T09:55:55.150293Z",
     "shell.execute_reply": "2022-11-13T09:55:55.149314Z"
    },
    "id": "VBFNNhj73EoO",
    "outputId": "e369a662-db8f-48ee-d0be-610f44b98863",
    "papermill": {
     "duration": 0.01464,
     "end_time": "2022-11-13T09:55:55.152647",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.138007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n",
    "# test_doc_vector = model.infer_vector(test_doc)\n",
    "# model.docvecs.most_similar(positive = [test_doc_vector])\n",
    "\n",
    "# '''\n",
    "# positive = List of sentences that contribute positively.\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc9b24",
   "metadata": {
    "id": "DCQ2T4zT5UON",
    "papermill": {
     "duration": 0.007363,
     "end_time": "2022-11-13T09:55:55.167823",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.160460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SentenceBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38cd612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.184678Z",
     "iopub.status.busy": "2022-11-13T09:55:55.183838Z",
     "iopub.status.idle": "2022-11-13T09:55:55.188885Z",
     "shell.execute_reply": "2022-11-13T09:55:55.188044Z"
    },
    "id": "d5Kd1-u23KXF",
    "outputId": "6b253cee-feee-4f25-ac1c-3d9ecec45b0f",
    "papermill": {
     "duration": 0.015625,
     "end_time": "2022-11-13T09:55:55.190965",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.175340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a60d6a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.207481Z",
     "iopub.status.busy": "2022-11-13T09:55:55.206728Z",
     "iopub.status.idle": "2022-11-13T09:55:55.210910Z",
     "shell.execute_reply": "2022-11-13T09:55:55.210019Z"
    },
    "id": "3SFHJsxw5aiM",
    "papermill": {
     "duration": 0.014589,
     "end_time": "2022-11-13T09:55:55.213101",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.198512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32f6afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.229526Z",
     "iopub.status.busy": "2022-11-13T09:55:55.228675Z",
     "iopub.status.idle": "2022-11-13T09:55:55.233180Z",
     "shell.execute_reply": "2022-11-13T09:55:55.232324Z"
    },
    "id": "6XHzXTfu5d16",
    "outputId": "51bc4973-62ac-4db6-e00f-a40a6c1f6d08",
    "papermill": {
     "duration": 0.01475,
     "end_time": "2022-11-13T09:55:55.235183",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.220433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentence_embeddings = sbert_model.encode(sentences)\n",
    "\n",
    "# print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "# print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n",
    "# sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbaa98da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.251436Z",
     "iopub.status.busy": "2022-11-13T09:55:55.250681Z",
     "iopub.status.idle": "2022-11-13T09:55:55.254835Z",
     "shell.execute_reply": "2022-11-13T09:55:55.253912Z"
    },
    "papermill": {
     "duration": 0.014298,
     "end_time": "2022-11-13T09:55:55.256944",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.242646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # keep the first 100 embeddings for each sentence in a separate list\n",
    "# embeddings = [sentence_embeddings[i][:100] for i in range(len(sentence_embeddings))]\n",
    "# embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da46356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.273480Z",
     "iopub.status.busy": "2022-11-13T09:55:55.272621Z",
     "iopub.status.idle": "2022-11-13T09:55:55.277358Z",
     "shell.execute_reply": "2022-11-13T09:55:55.276467Z"
    },
    "papermill": {
     "duration": 0.015062,
     "end_time": "2022-11-13T09:55:55.279399",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.264337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1feed432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.296183Z",
     "iopub.status.busy": "2022-11-13T09:55:55.295328Z",
     "iopub.status.idle": "2022-11-13T09:55:55.596697Z",
     "shell.execute_reply": "2022-11-13T09:55:55.595591Z"
    },
    "id": "TXzfmwFK6cDs",
    "papermill": {
     "duration": 0.311905,
     "end_time": "2022-11-13T09:55:55.598807",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.286902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  \n",
       "0                Adequate  \n",
       "1                Adequate  \n",
       "2                Adequate  \n",
       "3                Adequate  \n",
       "4                Adequate  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file train.csv and store it in a variable called train_data\n",
    "train_data = pd.read_csv('../input/feedback-prize-effectiveness/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfa1c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.616468Z",
     "iopub.status.busy": "2022-11-13T09:55:55.615676Z",
     "iopub.status.idle": "2022-11-13T09:55:55.625436Z",
     "shell.execute_reply": "2022-11-13T09:55:55.624482Z"
    },
    "papermill": {
     "duration": 0.020532,
     "end_time": "2022-11-13T09:55:55.627616",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.607084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of sentences from the train_data - train_data['discourse_text']\n",
    "train_sentences = train_data['discourse_text'].tolist()\n",
    "\n",
    "discourse_type = train_data['discourse_type'].tolist()\n",
    "# discourse_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1758e97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.645866Z",
     "iopub.status.busy": "2022-11-13T09:55:55.645134Z",
     "iopub.status.idle": "2022-11-13T09:55:55.650780Z",
     "shell.execute_reply": "2022-11-13T09:55:55.649691Z"
    },
    "papermill": {
     "duration": 0.017081,
     "end_time": "2022-11-13T09:55:55.653170",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.636089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def readCSV(filename):\n",
    "    rows = []\n",
    "    with open(filename, 'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8933a679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.672051Z",
     "iopub.status.busy": "2022-11-13T09:55:55.670102Z",
     "iopub.status.idle": "2022-11-13T09:55:55.848941Z",
     "shell.execute_reply": "2022-11-13T09:55:55.847017Z"
    },
    "papermill": {
     "duration": 0.190565,
     "end_time": "2022-11-13T09:55:55.851697",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.661132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37762 Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. \n",
      "37762\n",
      "37762\n"
     ]
    }
   ],
   "source": [
    "discourses = []\n",
    "dis_types = []\n",
    "dis_effectiveness = []\n",
    "rows = readCSV(\"../input/feedback-prize-effectiveness/train.csv\")\n",
    "for row in rows:\n",
    "    if len(row[2])<=510:\n",
    "        discourses.append(row[2])\n",
    "        dis_types.append(row[3])\n",
    "        dis_effectiveness.append(row[4])\n",
    "    while len(row[2])>510:\n",
    "        temp = row[2][:510]\n",
    "        row[2] = row[2][510:]\n",
    "        discourses.append(temp)\n",
    "        dis_types.append(row[3])\n",
    "        dis_effectiveness.append(row[4])\n",
    "print(len(discourses),discourses[0])\n",
    "print(len(dis_types))\n",
    "print(len(dis_effectiveness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3baa61a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.870315Z",
     "iopub.status.busy": "2022-11-13T09:55:55.869451Z",
     "iopub.status.idle": "2022-11-13T09:55:55.875249Z",
     "shell.execute_reply": "2022-11-13T09:55:55.874201Z"
    },
    "papermill": {
     "duration": 0.017155,
     "end_time": "2022-11-13T09:55:55.877408",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.860253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "typeDict = {'Lead':0,'Position':1,'Claim':2,'Counterclaim':3,'Rebuttal':4,'Evidence':5,'Concluding Statement':6}\n",
    "effectDict = {'Ineffective':0,'Adequate':1,'Effective':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4875560b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.894329Z",
     "iopub.status.busy": "2022-11-13T09:55:55.894040Z",
     "iopub.status.idle": "2022-11-13T09:55:55.897892Z",
     "shell.execute_reply": "2022-11-13T09:55:55.896975Z"
    },
    "papermill": {
     "duration": 0.01481,
     "end_time": "2022-11-13T09:55:55.900017",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.885207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dis_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "900dd8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.917139Z",
     "iopub.status.busy": "2022-11-13T09:55:55.916315Z",
     "iopub.status.idle": "2022-11-13T09:55:55.920641Z",
     "shell.execute_reply": "2022-11-13T09:55:55.919815Z"
    },
    "papermill": {
     "duration": 0.015089,
     "end_time": "2022-11-13T09:55:55.922794",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.907705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4122d97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:55:55.940167Z",
     "iopub.status.busy": "2022-11-13T09:55:55.939900Z",
     "iopub.status.idle": "2022-11-13T09:56:08.491943Z",
     "shell.execute_reply": "2022-11-13T09:56:08.490937Z"
    },
    "papermill": {
     "duration": 12.56362,
     "end_time": "2022-11-13T09:56:08.494567",
     "exception": false,
     "start_time": "2022-11-13T09:55:55.930947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import torch\n",
    "import transformers as ppb\n",
    "# from sent2vec.splitter import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "ROOT_DIR = os.getcwd() \n",
    "TEST_DIR = os.path.join(ROOT_DIR, 'test')\n",
    "DATA_DIR = os.path.join(TEST_DIR, 'dataset')\n",
    "\n",
    "ROOT_LOCAL = '/Users/pedramataee/'\n",
    "ROOT_LINUX = '~'\n",
    "\n",
    "WIKI_PATH = os.path.join(ROOT_LINUX, 'gensim-data/glove-wiki-gigaword-300') \n",
    "PRETRAINED_VECTORS_PATH_WIKI = os.path.join(WIKI_PATH, 'glove-wiki-gigaword-300.gz')\n",
    "\n",
    "FASTTEXT_NEWS_PATH = os.path.join(ROOT_LINUX, 'gensim-data/fasttext-wiki-news-subwords-300') \n",
    "PRETRAINED_VECTORS_PATH_FASTTEXT = os.path.join(FASTTEXT_NEWS_PATH, 'fasttext-wiki-news-subwords-300.gz')\n",
    "\n",
    "# Make sure to download \"en_core_web_sm\" package on your machine.\n",
    "# In case, you can run: \"python3 -m spacy download en_core_web_sm\"\n",
    "os.environ['LANGUAGE_MODEL_SPACY'] = \"en_core_web_sm\"\n",
    "\n",
    "\n",
    "class Splitter:\n",
    "    def __init__(self):\n",
    "        self.words = []\n",
    "        self.sentences = []\n",
    "        try:\n",
    "            self.nlp = spacy.load(os.environ['LANGUAGE_MODEL_SPACY'])\n",
    "        except Exception as error:\n",
    "            print(f'{error}\\n\\n Install \"en_core_web_sm\" in your environment. '\n",
    "                  f'Try running: \"python3 -m spacy download en_core_web_sm\" \\n '\n",
    "                  f'or follow instrucions here: https://spacy.io/usage')\n",
    "\n",
    "    def sent2words(self, sentences, **kwargs):\n",
    "        add_stop_words = kwargs.get('add_stop_words', [])\n",
    "        remove_stop_words = kwargs.get('remove_stop_words', [])\n",
    "\n",
    "        for w in add_stop_words:\n",
    "            self.nlp.vocab[w].is_stop = True\n",
    "        for w in remove_stop_words:\n",
    "            self.nlp.vocab[w].is_stop = False\n",
    "\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            doc = self.nlp(sentence.lower())\n",
    "            words.append([token.lemma_ for token in doc if not token.is_punct | token.is_space | token.is_stop])\n",
    "\n",
    "        self.words = words\n",
    "\n",
    "    def text2sents(self, texts):\n",
    "        for text in texts:\n",
    "            doc = self.nlp(text)\n",
    "            span = doc[0:5]\n",
    "            sents = list(doc.sents)\n",
    "            self.sentences.extend([sent for sent in sents])\n",
    "\n",
    "    def text2words(self, texts):\n",
    "        doc = self.nlp(texts)\n",
    "        tokenized_texts = []\n",
    "        for w in doc:\n",
    "            is_clean = w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num\n",
    "            if is_clean:\n",
    "                tokenized_texts.append(w.lemma_)\n",
    "\n",
    "        self.words = tokenized_texts\n",
    "\n",
    "\n",
    "def sentencizer_by_regex(texts):\n",
    "    alphabets = \"([A-Za-z])\"\n",
    "    prefixes = r\"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = r\"(Inc|Ltd|Jr|Sr|Co|etc)\"\n",
    "    starters = r\"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = r\"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = r\"[.](com|net|org|io|gov)\"\n",
    "\n",
    "    text = \" \" + texts + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
    "    text = re.sub(r\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n",
    "    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    pretrained_weights: str, default='distilbert-base-uncased'\n",
    "\n",
    "        If the string does not include an extension .txt, .gz or .bin, then Bert vectorizer is loaded using the specified weights.\n",
    "        Example: pass 'distilbert-base-multilingual-cased' to load Bert base multilingual model.\n",
    "\n",
    "        To load word2vec vectorizer pass a valid path to the weights file (.txt, .gz or .bin).\n",
    "        Example: pass 'glove-wiki-gigaword-300.gz' to load the Wiki vectors (when saved in the same folder you are running the code).\n",
    "\n",
    "    \n",
    "    ensemble_method: str, default='average'\n",
    "\n",
    "        How word vectors are computed into sentece vectors.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_weights = 'distilbert-base-uncased',\n",
    "                       ensemble_method = 'average'):\n",
    "        _, ext = os.path.splitext(pretrained_weights)\n",
    "        self.vectors = []\n",
    "        if not ext:\n",
    "            print(f'Initializing Bert {pretrained_weights}')\n",
    "            self.vectorizer = BertVectorizer(pretrained_weights=pretrained_weights)\n",
    "        else:\n",
    "            print(f'Initializing word2vec with vector path {pretrained_weights}')\n",
    "            self.vectorizer = GensimVectorizer(pretrained_weights=pretrained_weights, \n",
    "                                               ensemble_method=ensemble_method)\n",
    "\n",
    "    def run(self, sentences, remove_stop_words = ['not'], add_stop_words = []):\n",
    "        # SANITY CHECK\n",
    "        assert type(sentences) == list, 'A list must be passed!'\n",
    "        for sentence in sentences:\n",
    "            if type(sentence) != str:\n",
    "                raise TypeError(f'All items must be string type but {sentence} is type {type(sentence)}.')\n",
    "        # RUN\n",
    "        self.vectorizer._execute(sentences, remove_stop_words=remove_stop_words, add_stop_words=add_stop_words)\n",
    "        vectors = self.vectorizer.vectors\n",
    "        for idx in range(vectors.shape[0]):\n",
    "            self.vectors.append(vectors[idx])\n",
    "\n",
    "\n",
    "class BaseVectorizer():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.pretrained_weights = kwargs.get('pretrained_weights')\n",
    "        self.ensemble_method = kwargs.get('ensemble_method')\n",
    "        self.vectors = []\n",
    "    \n",
    "    def _load_model(self):\n",
    "        pass\n",
    "\n",
    "    def _execute(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BertVectorizer(BaseVectorizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'Vectorization done on {self.device}')\n",
    "        model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel,\n",
    "                                                            ppb.DistilBertTokenizer,\n",
    "                                                            self.pretrained_weights)\n",
    "        self.tokenizer = tokenizer_class.from_pretrained('../input/huggingface-bert/bert-base-uncased',local_files_only = True)\n",
    "        self.model = model_class.from_pretrained('../input/huggingface-bert/bert-base-uncased',local_files_only = True)\n",
    "    \n",
    "    def _execute(self, sentences, **kwargs):\n",
    "        model = self.model.to(self.device)\n",
    "        model.eval()\n",
    "        tokenized = list(map(lambda x: self.tokenizer.encode(x, add_special_tokens=True), sentences))\n",
    "        max_len = 0\n",
    "        for i in tokenized:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        # Move inputs to same device as model\n",
    "        input_ids = torch.tensor(np.array(padded)).type(torch.LongTensor).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids)\n",
    "        # Move vector results back to cpu if calculation was done on GPU\n",
    "        vectors = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        self.vectors = vectors\n",
    "\n",
    "class GensimVectorizer(BaseVectorizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        _, file_extension = os.path.splitext(self.pretrained_weights)\n",
    "        # Checks if file extension is binary\n",
    "        if file_extension == '.bin':\n",
    "            self.model = gensim.models.KeyedVectors.load_word2vec_format(self.pretrained_weights, binary=True)\n",
    "        elif file_extension == '.txt' or '.gz':\n",
    "            self.model = gensim.models.KeyedVectors.load_word2vec_format(self.pretrained_weights)\n",
    "        else:\n",
    "            raise IOError(f'The file extension {file_extension} is not valid. Word2vec valid formats are \".txt\" and \".bin\".')\n",
    "    \n",
    "    def _execute(self, sentences, **kwargs):\n",
    "        splitter = Splitter()\n",
    "        splitter.sent2words(sentences, remove_stop_words=kwargs.get('remove_stop_words'), add_stop_words=kwargs.get('add_stop_words'))\n",
    "        words = splitter.words\n",
    "        vectors = []\n",
    "        for element in words:\n",
    "            temp = []\n",
    "            for w in element:\n",
    "                temp.append(self.model[w])\n",
    "            if self.ensemble_method == 'average':\n",
    "                element_vec = np.mean(temp, axis=0)\n",
    "                try:\n",
    "                    vectors = np.vstack([vectors, element_vec])\n",
    "                except:\n",
    "                    vectors = element_vec\n",
    "        self.vectors = vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49334e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:56:08.511818Z",
     "iopub.status.busy": "2022-11-13T09:56:08.511330Z",
     "iopub.status.idle": "2022-11-13T09:56:08.522144Z",
     "shell.execute_reply": "2022-11-13T09:56:08.521160Z"
    },
    "papermill": {
     "duration": 0.021686,
     "end_time": "2022-11-13T09:56:08.524448",
     "exception": false,
     "start_time": "2022-11-13T09:56:08.502762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sent2vec.vectorizer import Vectorizer\n",
    "import numpy as np\n",
    "\n",
    "def getVectors(discourses, dis_types):\n",
    "    vectorizer = Vectorizer()\n",
    "    dis_vectors = []\n",
    "    for i in range(0,len(discourses),100):\n",
    "        vectorizer.run(discourses[i:min(i+100,len(discourses))], remove_stop_words=[], add_stop_words=[])\n",
    "        temp = vectorizer.vectors\n",
    "        for t in range(i,min(i+100,len(discourses))):\n",
    "            dis_vectors.append(temp[t][:100])\n",
    "    dis_vectors = np.array(dis_vectors)\n",
    "\n",
    "    # concatenate the type[dis_types[i]] and dis_vectors\n",
    "#     dis_vectors = np.concatenate((np.array([typeDict[dis_types[i]] for i in range(len(dis_types))]).reshape(-1,1),dis_vectors),axis=1)\n",
    "        \n",
    "\n",
    "#     print(len(dis_vectors))\n",
    "#     print(dis_vectors[0])\n",
    "    # return dis_vectors\n",
    "\n",
    "    output1 = []\n",
    "    output2 = []\n",
    "    for i in range(0,len(dis_types)):\n",
    "#         temp = [0 for i in range(0,len(typeDict))]\n",
    "#         temp[typeDict[dis_types[i]]]=1\n",
    "#         output1.append(temp)\n",
    "        output1.append(typeDict[dis_types[i]])\n",
    "    output1 = np.array(output1)\n",
    "#     print(len(output1))\n",
    "#     print(output1[0])\n",
    "    # make the output as  the one hot encoding of the effectDict[dis_effectiveness[i]]\n",
    "    for i in range(len(dis_effectiveness)):\n",
    "#         temp = [0,0,0]\n",
    "#         temp[effectDict[dis_effectiveness[i]]] = 1\n",
    "#         output2.append(temp)\n",
    "        output2.append(effectDict[dis_effectiveness[i]])\n",
    "    output2 = np.array(output2)\n",
    "#     print(len(output2))\n",
    "#     print(output2[0])\n",
    "    return dis_vectors,output1,output2\n",
    "\n",
    "# train_input, train_output = getVectors(discourses, dis_types)\n",
    "# getVectors(discourses, dis_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103955f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T09:56:08.541586Z",
     "iopub.status.busy": "2022-11-13T09:56:08.540608Z",
     "iopub.status.idle": "2022-11-13T10:01:45.347390Z",
     "shell.execute_reply": "2022-11-13T10:01:45.345884Z"
    },
    "papermill": {
     "duration": 336.819575,
     "end_time": "2022-11-13T10:01:45.351360",
     "exception": false,
     "start_time": "2022-11-13T09:56:08.531785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../input/huggingface-bert/bert-base-uncased were not used when initializing DistilBertModel: ['bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'cls.predictions.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at ../input/huggingface-bert/bert-base-uncased and are newly initialized: ['transformer.layer.6.attention.k_lin.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.9.ffn.lin1.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.q_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_input, train_output_1, train_output_2 = getVectors(discourses, dis_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3f7c546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T10:01:45.380739Z",
     "iopub.status.busy": "2022-11-13T10:01:45.380069Z",
     "iopub.status.idle": "2022-11-13T10:01:45.394351Z",
     "shell.execute_reply": "2022-11-13T10:01:45.393413Z"
    },
    "papermill": {
     "duration": 0.032009,
     "end_time": "2022-11-13T10:01:45.398323",
     "exception": false,
     "start_time": "2022-11-13T10:01:45.366314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c244a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T10:01:45.424373Z",
     "iopub.status.busy": "2022-11-13T10:01:45.423910Z",
     "iopub.status.idle": "2022-11-13T10:01:45.453869Z",
     "shell.execute_reply": "2022-11-13T10:01:45.452901Z"
    },
    "papermill": {
     "duration": 0.045552,
     "end_time": "2022-11-13T10:01:45.456719",
     "exception": false,
     "start_time": "2022-11-13T10:01:45.411167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.8367624884478817, 1: 0.5972354020370721, 2: 1.2801111902098377}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights1 = generate_class_weights(train_output_1)\n",
    "class_weights2 = generate_class_weights(train_output_2)\n",
    "class_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee4b7302",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-11-13T10:01:45.482644Z",
     "iopub.status.busy": "2022-11-13T10:01:45.482068Z",
     "iopub.status.idle": "2022-11-13T10:01:45.489942Z",
     "shell.execute_reply": "2022-11-13T10:01:45.489080Z"
    },
    "papermill": {
     "duration": 0.026164,
     "end_time": "2022-11-13T10:01:45.495510",
     "exception": false,
     "start_time": "2022-11-13T10:01:45.469346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(class_weight):\n",
    "    def loss(y_obs, y_pred):\n",
    "        y_obs = tf.dtypes.cast(y_obs, tf.int32)\n",
    "        hothot = tf.one_hot(tf.reshape(y_obs,[-1]),depth=len(class_weight))\n",
    "        weight = tf.math.multiply(class_weight, hothot)\n",
    "        weight = tf.reduce_sum(weight, axis=-1)\n",
    "        losses = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n",
    "            labels=y_obs, logits=y_pred, weights=weight\n",
    "        )\n",
    "        return losses\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c180fa67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T10:01:45.519765Z",
     "iopub.status.busy": "2022-11-13T10:01:45.519435Z",
     "iopub.status.idle": "2022-11-13T10:01:45.524417Z",
     "shell.execute_reply": "2022-11-13T10:01:45.523527Z"
    },
    "papermill": {
     "duration": 0.020266,
     "end_time": "2022-11-13T10:01:45.527001",
     "exception": false,
     "start_time": "2022-11-13T10:01:45.506735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss1 = weighted_categorical_crossentropy(list(class_weights1.values()))\n",
    "loss2 = weighted_categorical_crossentropy(list(class_weights2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aef36a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T10:01:45.551689Z",
     "iopub.status.busy": "2022-11-13T10:01:45.551399Z",
     "iopub.status.idle": "2022-11-13T10:01:45.556907Z",
     "shell.execute_reply": "2022-11-13T10:01:45.556068Z"
    },
    "papermill": {
     "duration": 0.023851,
     "end_time": "2022-11-13T10:01:45.562508",
     "exception": false,
     "start_time": "2022-11-13T10:01:45.538657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37762,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cc679ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T10:01:45.587144Z",
     "iopub.status.busy": "2022-11-13T10:01:45.586809Z",
     "iopub.status.idle": "2022-11-13T13:52:45.654154Z",
     "shell.execute_reply": "2022-11-13T13:52:45.653069Z"
    },
    "papermill": {
     "duration": 13860.082136,
     "end_time": "2022-11-13T13:52:45.656280",
     "exception": false,
     "start_time": "2022-11-13T10:01:45.574144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 10:01:46.572810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.573765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.574935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.575680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.576460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.577162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.578894: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 10:01:46.695747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.696664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.697457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.698193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.699239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:46.700056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.158366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.159250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.160040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.160813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.161561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.162235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10909 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-11-13 10:01:50.166510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 10:01:50.167217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 12839 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "2022-11-13 10:01:50.589323: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2518/2518 [==============================] - 16s 5ms/step - loss: 2.8686 - output_1_loss: 1.7978 - output_2_loss: 1.0708\n",
      "Epoch 2/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.7849 - output_1_loss: 1.7370 - output_2_loss: 1.0478\n",
      "Epoch 3/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.7691 - output_1_loss: 1.7315 - output_2_loss: 1.0378\n",
      "Epoch 4/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.7578 - output_1_loss: 1.7230 - output_2_loss: 1.0351\n",
      "Epoch 5/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.7508 - output_1_loss: 1.7193 - output_2_loss: 1.0314\n",
      "Epoch 6/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.7435 - output_1_loss: 1.7145 - output_2_loss: 1.0292\n",
      "Epoch 7/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.7390 - output_1_loss: 1.7111 - output_2_loss: 1.0277\n",
      "Epoch 8/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.7297 - output_1_loss: 1.7064 - output_2_loss: 1.0230\n",
      "Epoch 9/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.7227 - output_1_loss: 1.7013 - output_2_loss: 1.0215\n",
      "Epoch 10/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.7127 - output_1_loss: 1.6949 - output_2_loss: 1.0181\n",
      "Epoch 11/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.7076 - output_1_loss: 1.6903 - output_2_loss: 1.0174\n",
      "Epoch 12/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.7017 - output_1_loss: 1.6867 - output_2_loss: 1.0151\n",
      "Epoch 13/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6975 - output_1_loss: 1.6818 - output_2_loss: 1.0156\n",
      "Epoch 14/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6904 - output_1_loss: 1.6784 - output_2_loss: 1.0122\n",
      "Epoch 15/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6857 - output_1_loss: 1.6733 - output_2_loss: 1.0124\n",
      "Epoch 16/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6772 - output_1_loss: 1.6684 - output_2_loss: 1.0086\n",
      "Epoch 17/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6760 - output_1_loss: 1.6663 - output_2_loss: 1.0096\n",
      "Epoch 18/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6741 - output_1_loss: 1.6668 - output_2_loss: 1.0075\n",
      "Epoch 19/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6706 - output_1_loss: 1.6604 - output_2_loss: 1.0102\n",
      "Epoch 20/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6658 - output_1_loss: 1.6592 - output_2_loss: 1.0068\n",
      "Epoch 21/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6636 - output_1_loss: 1.6576 - output_2_loss: 1.0060\n",
      "Epoch 22/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6612 - output_1_loss: 1.6561 - output_2_loss: 1.0049\n",
      "Epoch 23/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6574 - output_1_loss: 1.6526 - output_2_loss: 1.0048\n",
      "Epoch 24/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6542 - output_1_loss: 1.6493 - output_2_loss: 1.0049\n",
      "Epoch 25/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6529 - output_1_loss: 1.6487 - output_2_loss: 1.0043\n",
      "Epoch 26/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6480 - output_1_loss: 1.6464 - output_2_loss: 1.0019\n",
      "Epoch 27/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6455 - output_1_loss: 1.6438 - output_2_loss: 1.0015\n",
      "Epoch 28/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6426 - output_1_loss: 1.6413 - output_2_loss: 1.0013\n",
      "Epoch 29/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6425 - output_1_loss: 1.6412 - output_2_loss: 1.0012\n",
      "Epoch 30/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6390 - output_1_loss: 1.6386 - output_2_loss: 1.0005\n",
      "Epoch 31/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.6366 - output_1_loss: 1.6379 - output_2_loss: 0.9985\n",
      "Epoch 32/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6327 - output_1_loss: 1.6329 - output_2_loss: 0.9998\n",
      "Epoch 33/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6317 - output_1_loss: 1.6304 - output_2_loss: 1.0013\n",
      "Epoch 34/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6276 - output_1_loss: 1.6299 - output_2_loss: 0.9979\n",
      "Epoch 35/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6263 - output_1_loss: 1.6276 - output_2_loss: 0.9986\n",
      "Epoch 36/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6216 - output_1_loss: 1.6248 - output_2_loss: 0.9970\n",
      "Epoch 37/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6216 - output_1_loss: 1.6243 - output_2_loss: 0.9972\n",
      "Epoch 38/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6201 - output_1_loss: 1.6232 - output_2_loss: 0.9967\n",
      "Epoch 39/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6182 - output_1_loss: 1.6209 - output_2_loss: 0.9973\n",
      "Epoch 40/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6122 - output_1_loss: 1.6166 - output_2_loss: 0.9955\n",
      "Epoch 41/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6119 - output_1_loss: 1.6162 - output_2_loss: 0.9956\n",
      "Epoch 42/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.6084 - output_1_loss: 1.6138 - output_2_loss: 0.9949\n",
      "Epoch 43/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6095 - output_1_loss: 1.6122 - output_2_loss: 0.9970\n",
      "Epoch 44/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.6082 - output_1_loss: 1.6122 - output_2_loss: 0.9959\n",
      "Epoch 45/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6046 - output_1_loss: 1.6103 - output_2_loss: 0.9945\n",
      "Epoch 46/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.6043 - output_1_loss: 1.6112 - output_2_loss: 0.9931\n",
      "Epoch 47/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.6049 - output_1_loss: 1.6107 - output_2_loss: 0.9942\n",
      "Epoch 48/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5997 - output_1_loss: 1.6078 - output_2_loss: 0.9921\n",
      "Epoch 49/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5951 - output_1_loss: 1.6031 - output_2_loss: 0.9921\n",
      "Epoch 50/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5919 - output_1_loss: 1.6004 - output_2_loss: 0.9913\n",
      "Epoch 51/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5973 - output_1_loss: 1.6044 - output_2_loss: 0.9929\n",
      "Epoch 52/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5914 - output_1_loss: 1.6001 - output_2_loss: 0.9913\n",
      "Epoch 53/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5912 - output_1_loss: 1.5998 - output_2_loss: 0.9913\n",
      "Epoch 54/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5903 - output_1_loss: 1.5980 - output_2_loss: 0.9919\n",
      "Epoch 55/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5882 - output_1_loss: 1.5965 - output_2_loss: 0.9915\n",
      "Epoch 56/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5878 - output_1_loss: 1.5963 - output_2_loss: 0.9919\n",
      "Epoch 57/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5832 - output_1_loss: 1.5939 - output_2_loss: 0.9893\n",
      "Epoch 58/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5808 - output_1_loss: 1.5911 - output_2_loss: 0.9897\n",
      "Epoch 59/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5859 - output_1_loss: 1.5953 - output_2_loss: 0.9912\n",
      "Epoch 60/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5793 - output_1_loss: 1.5912 - output_2_loss: 0.9880\n",
      "Epoch 61/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5808 - output_1_loss: 1.5923 - output_2_loss: 0.9884\n",
      "Epoch 62/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5791 - output_1_loss: 1.5905 - output_2_loss: 0.9889\n",
      "Epoch 63/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5744 - output_1_loss: 1.5856 - output_2_loss: 0.9887\n",
      "Epoch 64/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5777 - output_1_loss: 1.5895 - output_2_loss: 0.9881\n",
      "Epoch 65/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5734 - output_1_loss: 1.5841 - output_2_loss: 0.9892\n",
      "Epoch 66/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5730 - output_1_loss: 1.5855 - output_2_loss: 0.9877\n",
      "Epoch 67/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5723 - output_1_loss: 1.5845 - output_2_loss: 0.9877\n",
      "Epoch 68/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5721 - output_1_loss: 1.5853 - output_2_loss: 0.9868\n",
      "Epoch 69/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5694 - output_1_loss: 1.5822 - output_2_loss: 0.9871\n",
      "Epoch 70/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5725 - output_1_loss: 1.5843 - output_2_loss: 0.9881\n",
      "Epoch 71/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5688 - output_1_loss: 1.5823 - output_2_loss: 0.9865\n",
      "Epoch 72/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5681 - output_1_loss: 1.5808 - output_2_loss: 0.9875\n",
      "Epoch 73/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5643 - output_1_loss: 1.5784 - output_2_loss: 0.9862\n",
      "Epoch 74/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5649 - output_1_loss: 1.5785 - output_2_loss: 0.9864\n",
      "Epoch 75/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5673 - output_1_loss: 1.5801 - output_2_loss: 0.9870\n",
      "Epoch 76/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5648 - output_1_loss: 1.5768 - output_2_loss: 0.9878\n",
      "Epoch 77/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5676 - output_1_loss: 1.5800 - output_2_loss: 0.9876\n",
      "Epoch 78/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5595 - output_1_loss: 1.5736 - output_2_loss: 0.9858\n",
      "Epoch 79/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5565 - output_1_loss: 1.5705 - output_2_loss: 0.9859\n",
      "Epoch 80/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5621 - output_1_loss: 1.5767 - output_2_loss: 0.9853\n",
      "Epoch 81/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5587 - output_1_loss: 1.5734 - output_2_loss: 0.9855\n",
      "Epoch 82/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5572 - output_1_loss: 1.5721 - output_2_loss: 0.9849\n",
      "Epoch 83/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5554 - output_1_loss: 1.5708 - output_2_loss: 0.9848\n",
      "Epoch 84/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5590 - output_1_loss: 1.5748 - output_2_loss: 0.9841\n",
      "Epoch 85/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5581 - output_1_loss: 1.5730 - output_2_loss: 0.9850\n",
      "Epoch 86/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5577 - output_1_loss: 1.5723 - output_2_loss: 0.9853\n",
      "Epoch 87/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5532 - output_1_loss: 1.5684 - output_2_loss: 0.9848\n",
      "Epoch 88/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5513 - output_1_loss: 1.5664 - output_2_loss: 0.9849\n",
      "Epoch 89/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5504 - output_1_loss: 1.5671 - output_2_loss: 0.9831\n",
      "Epoch 90/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5518 - output_1_loss: 1.5670 - output_2_loss: 0.9849\n",
      "Epoch 91/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5501 - output_1_loss: 1.5679 - output_2_loss: 0.9820\n",
      "Epoch 92/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5471 - output_1_loss: 1.5643 - output_2_loss: 0.9829\n",
      "Epoch 93/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5520 - output_1_loss: 1.5683 - output_2_loss: 0.9837\n",
      "Epoch 94/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5471 - output_1_loss: 1.5646 - output_2_loss: 0.9825\n",
      "Epoch 95/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5486 - output_1_loss: 1.5655 - output_2_loss: 0.9834\n",
      "Epoch 96/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5435 - output_1_loss: 1.5623 - output_2_loss: 0.9812\n",
      "Epoch 97/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5447 - output_1_loss: 1.5622 - output_2_loss: 0.9830\n",
      "Epoch 98/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5491 - output_1_loss: 1.5650 - output_2_loss: 0.9840\n",
      "Epoch 99/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5433 - output_1_loss: 1.5620 - output_2_loss: 0.9815\n",
      "Epoch 100/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5443 - output_1_loss: 1.5625 - output_2_loss: 0.9820\n",
      "Epoch 101/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5435 - output_1_loss: 1.5616 - output_2_loss: 0.9819\n",
      "Epoch 102/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5446 - output_1_loss: 1.5622 - output_2_loss: 0.9822\n",
      "Epoch 103/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5444 - output_1_loss: 1.5613 - output_2_loss: 0.9829\n",
      "Epoch 104/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5444 - output_1_loss: 1.5620 - output_2_loss: 0.9822\n",
      "Epoch 105/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5435 - output_1_loss: 1.5619 - output_2_loss: 0.9816\n",
      "Epoch 106/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5387 - output_1_loss: 1.5573 - output_2_loss: 0.9812\n",
      "Epoch 107/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5399 - output_1_loss: 1.5564 - output_2_loss: 0.9836\n",
      "Epoch 108/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5407 - output_1_loss: 1.5581 - output_2_loss: 0.9824\n",
      "Epoch 109/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5358 - output_1_loss: 1.5543 - output_2_loss: 0.9815\n",
      "Epoch 110/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5386 - output_1_loss: 1.5571 - output_2_loss: 0.9815\n",
      "Epoch 111/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5341 - output_1_loss: 1.5526 - output_2_loss: 0.9814\n",
      "Epoch 112/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5336 - output_1_loss: 1.5546 - output_2_loss: 0.9795\n",
      "Epoch 113/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5331 - output_1_loss: 1.5532 - output_2_loss: 0.9799\n",
      "Epoch 114/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5361 - output_1_loss: 1.5553 - output_2_loss: 0.9810\n",
      "Epoch 115/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5330 - output_1_loss: 1.5531 - output_2_loss: 0.9801\n",
      "Epoch 116/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5353 - output_1_loss: 1.5534 - output_2_loss: 0.9817\n",
      "Epoch 117/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5363 - output_1_loss: 1.5561 - output_2_loss: 0.9803\n",
      "Epoch 118/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5343 - output_1_loss: 1.5535 - output_2_loss: 0.9807\n",
      "Epoch 119/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5329 - output_1_loss: 1.5527 - output_2_loss: 0.9802\n",
      "Epoch 120/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5371 - output_1_loss: 1.5558 - output_2_loss: 0.9813\n",
      "Epoch 121/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5296 - output_1_loss: 1.5482 - output_2_loss: 0.9814\n",
      "Epoch 122/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5315 - output_1_loss: 1.5519 - output_2_loss: 0.9794\n",
      "Epoch 123/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5339 - output_1_loss: 1.5536 - output_2_loss: 0.9804\n",
      "Epoch 124/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5293 - output_1_loss: 1.5500 - output_2_loss: 0.9795\n",
      "Epoch 125/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5341 - output_1_loss: 1.5524 - output_2_loss: 0.9817\n",
      "Epoch 126/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5287 - output_1_loss: 1.5495 - output_2_loss: 0.9792\n",
      "Epoch 127/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5265 - output_1_loss: 1.5481 - output_2_loss: 0.9785\n",
      "Epoch 128/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5295 - output_1_loss: 1.5497 - output_2_loss: 0.9797\n",
      "Epoch 129/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5275 - output_1_loss: 1.5484 - output_2_loss: 0.9793\n",
      "Epoch 130/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5281 - output_1_loss: 1.5488 - output_2_loss: 0.9793\n",
      "Epoch 131/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5295 - output_1_loss: 1.5497 - output_2_loss: 0.9797\n",
      "Epoch 132/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5274 - output_1_loss: 1.5473 - output_2_loss: 0.9801\n",
      "Epoch 133/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5253 - output_1_loss: 1.5474 - output_2_loss: 0.9780\n",
      "Epoch 134/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5246 - output_1_loss: 1.5463 - output_2_loss: 0.9783\n",
      "Epoch 135/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5268 - output_1_loss: 1.5490 - output_2_loss: 0.9779\n",
      "Epoch 136/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5255 - output_1_loss: 1.5471 - output_2_loss: 0.9782\n",
      "Epoch 137/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5224 - output_1_loss: 1.5443 - output_2_loss: 0.9781\n",
      "Epoch 138/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5263 - output_1_loss: 1.5460 - output_2_loss: 0.9804\n",
      "Epoch 139/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5239 - output_1_loss: 1.5463 - output_2_loss: 0.9777\n",
      "Epoch 140/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5202 - output_1_loss: 1.5416 - output_2_loss: 0.9788\n",
      "Epoch 141/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5198 - output_1_loss: 1.5425 - output_2_loss: 0.9772\n",
      "Epoch 142/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5233 - output_1_loss: 1.5440 - output_2_loss: 0.9793\n",
      "Epoch 143/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5222 - output_1_loss: 1.5441 - output_2_loss: 0.9785\n",
      "Epoch 144/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5212 - output_1_loss: 1.5427 - output_2_loss: 0.9782\n",
      "Epoch 145/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5192 - output_1_loss: 1.5414 - output_2_loss: 0.9779\n",
      "Epoch 146/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5179 - output_1_loss: 1.5399 - output_2_loss: 0.9778\n",
      "Epoch 147/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5196 - output_1_loss: 1.5419 - output_2_loss: 0.9776\n",
      "Epoch 148/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5166 - output_1_loss: 1.5405 - output_2_loss: 0.9760\n",
      "Epoch 149/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5195 - output_1_loss: 1.5408 - output_2_loss: 0.9786\n",
      "Epoch 150/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5230 - output_1_loss: 1.5449 - output_2_loss: 0.9779\n",
      "Epoch 151/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5167 - output_1_loss: 1.5391 - output_2_loss: 0.9777\n",
      "Epoch 152/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5176 - output_1_loss: 1.5404 - output_2_loss: 0.9771\n",
      "Epoch 153/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5169 - output_1_loss: 1.5400 - output_2_loss: 0.9770\n",
      "Epoch 154/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5150 - output_1_loss: 1.5386 - output_2_loss: 0.9764\n",
      "Epoch 155/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5164 - output_1_loss: 1.5397 - output_2_loss: 0.9767\n",
      "Epoch 156/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5149 - output_1_loss: 1.5387 - output_2_loss: 0.9762\n",
      "Epoch 157/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5119 - output_1_loss: 1.5354 - output_2_loss: 0.9770\n",
      "Epoch 158/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5156 - output_1_loss: 1.5375 - output_2_loss: 0.9778\n",
      "Epoch 159/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5152 - output_1_loss: 1.5395 - output_2_loss: 0.9757\n",
      "Epoch 160/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5128 - output_1_loss: 1.5366 - output_2_loss: 0.9762\n",
      "Epoch 161/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5157 - output_1_loss: 1.5386 - output_2_loss: 0.9768\n",
      "Epoch 162/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5159 - output_1_loss: 1.5401 - output_2_loss: 0.9759\n",
      "Epoch 163/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5117 - output_1_loss: 1.5338 - output_2_loss: 0.9778\n",
      "Epoch 164/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5147 - output_1_loss: 1.5396 - output_2_loss: 0.9751\n",
      "Epoch 165/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5102 - output_1_loss: 1.5346 - output_2_loss: 0.9758\n",
      "Epoch 166/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5127 - output_1_loss: 1.5356 - output_2_loss: 0.9769\n",
      "Epoch 167/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5135 - output_1_loss: 1.5376 - output_2_loss: 0.9758\n",
      "Epoch 168/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5102 - output_1_loss: 1.5337 - output_2_loss: 0.9768\n",
      "Epoch 169/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5126 - output_1_loss: 1.5373 - output_2_loss: 0.9754\n",
      "Epoch 170/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5116 - output_1_loss: 1.5370 - output_2_loss: 0.9748\n",
      "Epoch 171/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5094 - output_1_loss: 1.5336 - output_2_loss: 0.9757\n",
      "Epoch 172/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5096 - output_1_loss: 1.5332 - output_2_loss: 0.9765\n",
      "Epoch 173/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5090 - output_1_loss: 1.5336 - output_2_loss: 0.9753\n",
      "Epoch 174/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5069 - output_1_loss: 1.5323 - output_2_loss: 0.9747\n",
      "Epoch 175/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5096 - output_1_loss: 1.5333 - output_2_loss: 0.9762\n",
      "Epoch 176/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5121 - output_1_loss: 1.5351 - output_2_loss: 0.9771\n",
      "Epoch 177/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5081 - output_1_loss: 1.5313 - output_2_loss: 0.9769\n",
      "Epoch 178/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5059 - output_1_loss: 1.5307 - output_2_loss: 0.9750\n",
      "Epoch 179/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5070 - output_1_loss: 1.5325 - output_2_loss: 0.9745\n",
      "Epoch 180/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5080 - output_1_loss: 1.5334 - output_2_loss: 0.9744\n",
      "Epoch 181/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5040 - output_1_loss: 1.5290 - output_2_loss: 0.9749\n",
      "Epoch 182/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5041 - output_1_loss: 1.5286 - output_2_loss: 0.9756\n",
      "Epoch 183/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5062 - output_1_loss: 1.5306 - output_2_loss: 0.9756\n",
      "Epoch 184/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5111 - output_1_loss: 1.5355 - output_2_loss: 0.9757\n",
      "Epoch 185/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5060 - output_1_loss: 1.5308 - output_2_loss: 0.9750\n",
      "Epoch 186/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.5027 - output_1_loss: 1.5288 - output_2_loss: 0.9742\n",
      "Epoch 187/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5041 - output_1_loss: 1.5281 - output_2_loss: 0.9760\n",
      "Epoch 188/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5074 - output_1_loss: 1.5319 - output_2_loss: 0.9754\n",
      "Epoch 189/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5038 - output_1_loss: 1.5282 - output_2_loss: 0.9755\n",
      "Epoch 190/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5041 - output_1_loss: 1.5301 - output_2_loss: 0.9740\n",
      "Epoch 191/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5044 - output_1_loss: 1.5309 - output_2_loss: 0.9736\n",
      "Epoch 192/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5038 - output_1_loss: 1.5298 - output_2_loss: 0.9741\n",
      "Epoch 193/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5034 - output_1_loss: 1.5295 - output_2_loss: 0.9739\n",
      "Epoch 194/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5047 - output_1_loss: 1.5314 - output_2_loss: 0.9731\n",
      "Epoch 195/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5052 - output_1_loss: 1.5311 - output_2_loss: 0.9741\n",
      "Epoch 196/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4991 - output_1_loss: 1.5249 - output_2_loss: 0.9744\n",
      "Epoch 197/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5029 - output_1_loss: 1.5288 - output_2_loss: 0.9738\n",
      "Epoch 198/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5026 - output_1_loss: 1.5284 - output_2_loss: 0.9744\n",
      "Epoch 199/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5015 - output_1_loss: 1.5285 - output_2_loss: 0.9728\n",
      "Epoch 200/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5020 - output_1_loss: 1.5264 - output_2_loss: 0.9754\n",
      "Epoch 201/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5006 - output_1_loss: 1.5270 - output_2_loss: 0.9735\n",
      "Epoch 202/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5007 - output_1_loss: 1.5274 - output_2_loss: 0.9733\n",
      "Epoch 203/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4982 - output_1_loss: 1.5265 - output_2_loss: 0.9718\n",
      "Epoch 204/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4985 - output_1_loss: 1.5247 - output_2_loss: 0.9736\n",
      "Epoch 205/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.5002 - output_1_loss: 1.5274 - output_2_loss: 0.9731\n",
      "Epoch 206/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.5015 - output_1_loss: 1.5280 - output_2_loss: 0.9734\n",
      "Epoch 207/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4992 - output_1_loss: 1.5250 - output_2_loss: 0.9741\n",
      "Epoch 208/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4979 - output_1_loss: 1.5244 - output_2_loss: 0.9733\n",
      "Epoch 209/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.5004 - output_1_loss: 1.5254 - output_2_loss: 0.9750\n",
      "Epoch 210/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5005 - output_1_loss: 1.5271 - output_2_loss: 0.9737\n",
      "Epoch 211/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4948 - output_1_loss: 1.5234 - output_2_loss: 0.9714\n",
      "Epoch 212/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4981 - output_1_loss: 1.5247 - output_2_loss: 0.9736\n",
      "Epoch 213/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4957 - output_1_loss: 1.5218 - output_2_loss: 0.9741\n",
      "Epoch 214/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4980 - output_1_loss: 1.5243 - output_2_loss: 0.9740\n",
      "Epoch 215/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4991 - output_1_loss: 1.5261 - output_2_loss: 0.9731\n",
      "Epoch 216/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4968 - output_1_loss: 1.5239 - output_2_loss: 0.9728\n",
      "Epoch 217/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4976 - output_1_loss: 1.5237 - output_2_loss: 0.9737\n",
      "Epoch 218/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4990 - output_1_loss: 1.5250 - output_2_loss: 0.9740\n",
      "Epoch 219/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4961 - output_1_loss: 1.5219 - output_2_loss: 0.9743\n",
      "Epoch 220/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4948 - output_1_loss: 1.5211 - output_2_loss: 0.9735\n",
      "Epoch 221/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4991 - output_1_loss: 1.5268 - output_2_loss: 0.9724\n",
      "Epoch 222/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4984 - output_1_loss: 1.5239 - output_2_loss: 0.9743\n",
      "Epoch 223/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4951 - output_1_loss: 1.5239 - output_2_loss: 0.9716\n",
      "Epoch 224/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.5007 - output_1_loss: 1.5283 - output_2_loss: 0.9724\n",
      "Epoch 225/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4939 - output_1_loss: 1.5216 - output_2_loss: 0.9722\n",
      "Epoch 226/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4985 - output_1_loss: 1.5253 - output_2_loss: 0.9733\n",
      "Epoch 227/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4951 - output_1_loss: 1.5226 - output_2_loss: 0.9726\n",
      "Epoch 228/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4957 - output_1_loss: 1.5226 - output_2_loss: 0.9732\n",
      "Epoch 229/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4939 - output_1_loss: 1.5208 - output_2_loss: 0.9732\n",
      "Epoch 230/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4946 - output_1_loss: 1.5208 - output_2_loss: 0.9738\n",
      "Epoch 231/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4964 - output_1_loss: 1.5239 - output_2_loss: 0.9724\n",
      "Epoch 232/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4934 - output_1_loss: 1.5205 - output_2_loss: 0.9728\n",
      "Epoch 233/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4914 - output_1_loss: 1.5203 - output_2_loss: 0.9711\n",
      "Epoch 234/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4956 - output_1_loss: 1.5240 - output_2_loss: 0.9716\n",
      "Epoch 235/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4952 - output_1_loss: 1.5220 - output_2_loss: 0.9731\n",
      "Epoch 236/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4922 - output_1_loss: 1.5208 - output_2_loss: 0.9713\n",
      "Epoch 237/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4933 - output_1_loss: 1.5213 - output_2_loss: 0.9720\n",
      "Epoch 238/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4917 - output_1_loss: 1.5207 - output_2_loss: 0.9710\n",
      "Epoch 239/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4926 - output_1_loss: 1.5177 - output_2_loss: 0.9747\n",
      "Epoch 240/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4962 - output_1_loss: 1.5236 - output_2_loss: 0.9726\n",
      "Epoch 241/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4899 - output_1_loss: 1.5184 - output_2_loss: 0.9716\n",
      "Epoch 242/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4937 - output_1_loss: 1.5216 - output_2_loss: 0.9720\n",
      "Epoch 243/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4901 - output_1_loss: 1.5181 - output_2_loss: 0.9720\n",
      "Epoch 244/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4906 - output_1_loss: 1.5188 - output_2_loss: 0.9716\n",
      "Epoch 245/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4926 - output_1_loss: 1.5211 - output_2_loss: 0.9718\n",
      "Epoch 246/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4869 - output_1_loss: 1.5167 - output_2_loss: 0.9700\n",
      "Epoch 247/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4923 - output_1_loss: 1.5193 - output_2_loss: 0.9728\n",
      "Epoch 248/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4919 - output_1_loss: 1.5208 - output_2_loss: 0.9710\n",
      "Epoch 249/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4887 - output_1_loss: 1.5156 - output_2_loss: 0.9731\n",
      "Epoch 250/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4937 - output_1_loss: 1.5213 - output_2_loss: 0.9724\n",
      "Epoch 251/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4875 - output_1_loss: 1.5151 - output_2_loss: 0.9724\n",
      "Epoch 252/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4906 - output_1_loss: 1.5193 - output_2_loss: 0.9713\n",
      "Epoch 253/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4907 - output_1_loss: 1.5192 - output_2_loss: 0.9712\n",
      "Epoch 254/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4859 - output_1_loss: 1.5159 - output_2_loss: 0.9702\n",
      "Epoch 255/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4920 - output_1_loss: 1.5206 - output_2_loss: 0.9714\n",
      "Epoch 256/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4886 - output_1_loss: 1.5178 - output_2_loss: 0.9710\n",
      "Epoch 257/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4877 - output_1_loss: 1.5166 - output_2_loss: 0.9711\n",
      "Epoch 258/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4895 - output_1_loss: 1.5189 - output_2_loss: 0.9706\n",
      "Epoch 259/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4850 - output_1_loss: 1.5149 - output_2_loss: 0.9703\n",
      "Epoch 260/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4883 - output_1_loss: 1.5178 - output_2_loss: 0.9705\n",
      "Epoch 261/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4887 - output_1_loss: 1.5165 - output_2_loss: 0.9720\n",
      "Epoch 262/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4909 - output_1_loss: 1.5197 - output_2_loss: 0.9714\n",
      "Epoch 263/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4864 - output_1_loss: 1.5141 - output_2_loss: 0.9721\n",
      "Epoch 264/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4923 - output_1_loss: 1.5208 - output_2_loss: 0.9718\n",
      "Epoch 265/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4893 - output_1_loss: 1.5186 - output_2_loss: 0.9713\n",
      "Epoch 266/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4855 - output_1_loss: 1.5149 - output_2_loss: 0.9706\n",
      "Epoch 267/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4867 - output_1_loss: 1.5135 - output_2_loss: 0.9730\n",
      "Epoch 268/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4869 - output_1_loss: 1.5152 - output_2_loss: 0.9715\n",
      "Epoch 269/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4922 - output_1_loss: 1.5208 - output_2_loss: 0.9717\n",
      "Epoch 270/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4857 - output_1_loss: 1.5160 - output_2_loss: 0.9696\n",
      "Epoch 271/1000\n",
      "2518/2518 [==============================] - 16s 7ms/step - loss: 2.4846 - output_1_loss: 1.5121 - output_2_loss: 0.9726\n",
      "Epoch 272/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4845 - output_1_loss: 1.5137 - output_2_loss: 0.9707\n",
      "Epoch 273/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4868 - output_1_loss: 1.5151 - output_2_loss: 0.9719\n",
      "Epoch 274/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4873 - output_1_loss: 1.5163 - output_2_loss: 0.9710\n",
      "Epoch 275/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4868 - output_1_loss: 1.5149 - output_2_loss: 0.9717\n",
      "Epoch 276/1000\n",
      "2518/2518 [==============================] - 16s 7ms/step - loss: 2.4861 - output_1_loss: 1.5154 - output_2_loss: 0.9707\n",
      "Epoch 277/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4878 - output_1_loss: 1.5166 - output_2_loss: 0.9715\n",
      "Epoch 278/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4858 - output_1_loss: 1.5135 - output_2_loss: 0.9723\n",
      "Epoch 279/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4847 - output_1_loss: 1.5139 - output_2_loss: 0.9707\n",
      "Epoch 280/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4842 - output_1_loss: 1.5128 - output_2_loss: 0.9712\n",
      "Epoch 281/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4868 - output_1_loss: 1.5161 - output_2_loss: 0.9709\n",
      "Epoch 282/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4830 - output_1_loss: 1.5122 - output_2_loss: 0.9705\n",
      "Epoch 283/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4854 - output_1_loss: 1.5135 - output_2_loss: 0.9720\n",
      "Epoch 284/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4851 - output_1_loss: 1.5143 - output_2_loss: 0.9707\n",
      "Epoch 285/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4839 - output_1_loss: 1.5124 - output_2_loss: 0.9714\n",
      "Epoch 286/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4843 - output_1_loss: 1.5141 - output_2_loss: 0.9700\n",
      "Epoch 287/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4842 - output_1_loss: 1.5121 - output_2_loss: 0.9721\n",
      "Epoch 288/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4879 - output_1_loss: 1.5162 - output_2_loss: 0.9717\n",
      "Epoch 289/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4856 - output_1_loss: 1.5134 - output_2_loss: 0.9722\n",
      "Epoch 290/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4859 - output_1_loss: 1.5139 - output_2_loss: 0.9719\n",
      "Epoch 291/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4853 - output_1_loss: 1.5132 - output_2_loss: 0.9720\n",
      "Epoch 292/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4837 - output_1_loss: 1.5134 - output_2_loss: 0.9708\n",
      "Epoch 293/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4843 - output_1_loss: 1.5130 - output_2_loss: 0.9712\n",
      "Epoch 294/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4880 - output_1_loss: 1.5167 - output_2_loss: 0.9715\n",
      "Epoch 295/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4839 - output_1_loss: 1.5118 - output_2_loss: 0.9721\n",
      "Epoch 296/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4853 - output_1_loss: 1.5141 - output_2_loss: 0.9711\n",
      "Epoch 297/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4832 - output_1_loss: 1.5126 - output_2_loss: 0.9704\n",
      "Epoch 298/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4817 - output_1_loss: 1.5100 - output_2_loss: 0.9716\n",
      "Epoch 299/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4805 - output_1_loss: 1.5102 - output_2_loss: 0.9700\n",
      "Epoch 300/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4809 - output_1_loss: 1.5110 - output_2_loss: 0.9697\n",
      "Epoch 301/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4867 - output_1_loss: 1.5154 - output_2_loss: 0.9716\n",
      "Epoch 302/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4802 - output_1_loss: 1.5091 - output_2_loss: 0.9713\n",
      "Epoch 303/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4791 - output_1_loss: 1.5086 - output_2_loss: 0.9709\n",
      "Epoch 304/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4805 - output_1_loss: 1.5095 - output_2_loss: 0.9709\n",
      "Epoch 305/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4805 - output_1_loss: 1.5099 - output_2_loss: 0.9704\n",
      "Epoch 306/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4820 - output_1_loss: 1.5090 - output_2_loss: 0.9730\n",
      "Epoch 307/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4816 - output_1_loss: 1.5114 - output_2_loss: 0.9701\n",
      "Epoch 308/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4827 - output_1_loss: 1.5134 - output_2_loss: 0.9693\n",
      "Epoch 309/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4816 - output_1_loss: 1.5106 - output_2_loss: 0.9710\n",
      "Epoch 310/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4835 - output_1_loss: 1.5130 - output_2_loss: 0.9704\n",
      "Epoch 311/1000\n",
      "2518/2518 [==============================] - 16s 7ms/step - loss: 2.4809 - output_1_loss: 1.5098 - output_2_loss: 0.9710\n",
      "Epoch 312/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4812 - output_1_loss: 1.5107 - output_2_loss: 0.9704\n",
      "Epoch 313/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4829 - output_1_loss: 1.5110 - output_2_loss: 0.9719\n",
      "Epoch 314/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4798 - output_1_loss: 1.5101 - output_2_loss: 0.9695\n",
      "Epoch 315/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4805 - output_1_loss: 1.5092 - output_2_loss: 0.9715\n",
      "Epoch 316/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4827 - output_1_loss: 1.5121 - output_2_loss: 0.9705\n",
      "Epoch 317/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4777 - output_1_loss: 1.5073 - output_2_loss: 0.9705\n",
      "Epoch 318/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4809 - output_1_loss: 1.5095 - output_2_loss: 0.9714\n",
      "Epoch 319/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4807 - output_1_loss: 1.5098 - output_2_loss: 0.9713\n",
      "Epoch 320/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4783 - output_1_loss: 1.5080 - output_2_loss: 0.9702\n",
      "Epoch 321/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4788 - output_1_loss: 1.5092 - output_2_loss: 0.9694\n",
      "Epoch 322/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4817 - output_1_loss: 1.5112 - output_2_loss: 0.9706\n",
      "Epoch 323/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4795 - output_1_loss: 1.5100 - output_2_loss: 0.9693\n",
      "Epoch 324/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4759 - output_1_loss: 1.5064 - output_2_loss: 0.9694\n",
      "Epoch 325/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4792 - output_1_loss: 1.5086 - output_2_loss: 0.9706\n",
      "Epoch 326/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4798 - output_1_loss: 1.5095 - output_2_loss: 0.9702\n",
      "Epoch 327/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4794 - output_1_loss: 1.5086 - output_2_loss: 0.9707\n",
      "Epoch 328/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4788 - output_1_loss: 1.5100 - output_2_loss: 0.9689\n",
      "Epoch 329/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4779 - output_1_loss: 1.5078 - output_2_loss: 0.9702\n",
      "Epoch 330/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4774 - output_1_loss: 1.5069 - output_2_loss: 0.9706\n",
      "Epoch 331/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4747 - output_1_loss: 1.5047 - output_2_loss: 0.9698\n",
      "Epoch 332/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4832 - output_1_loss: 1.5121 - output_2_loss: 0.9711\n",
      "Epoch 333/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4764 - output_1_loss: 1.5063 - output_2_loss: 0.9704\n",
      "Epoch 334/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4795 - output_1_loss: 1.5087 - output_2_loss: 0.9706\n",
      "Epoch 335/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4775 - output_1_loss: 1.5080 - output_2_loss: 0.9698\n",
      "Epoch 336/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4781 - output_1_loss: 1.5078 - output_2_loss: 0.9701\n",
      "Epoch 337/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4753 - output_1_loss: 1.5052 - output_2_loss: 0.9699\n",
      "Epoch 338/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4752 - output_1_loss: 1.5056 - output_2_loss: 0.9697\n",
      "Epoch 339/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4816 - output_1_loss: 1.5100 - output_2_loss: 0.9716\n",
      "Epoch 340/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4779 - output_1_loss: 1.5075 - output_2_loss: 0.9702\n",
      "Epoch 341/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4758 - output_1_loss: 1.5057 - output_2_loss: 0.9699\n",
      "Epoch 342/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4736 - output_1_loss: 1.5056 - output_2_loss: 0.9680\n",
      "Epoch 343/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4789 - output_1_loss: 1.5098 - output_2_loss: 0.9692\n",
      "Epoch 344/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4756 - output_1_loss: 1.5065 - output_2_loss: 0.9689\n",
      "Epoch 345/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4777 - output_1_loss: 1.5075 - output_2_loss: 0.9701\n",
      "Epoch 346/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4722 - output_1_loss: 1.5043 - output_2_loss: 0.9679\n",
      "Epoch 347/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4749 - output_1_loss: 1.5044 - output_2_loss: 0.9707\n",
      "Epoch 348/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4752 - output_1_loss: 1.5054 - output_2_loss: 0.9698\n",
      "Epoch 349/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4765 - output_1_loss: 1.5075 - output_2_loss: 0.9689\n",
      "Epoch 350/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4728 - output_1_loss: 1.5036 - output_2_loss: 0.9689\n",
      "Epoch 351/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4746 - output_1_loss: 1.5045 - output_2_loss: 0.9701\n",
      "Epoch 352/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4756 - output_1_loss: 1.5066 - output_2_loss: 0.9688\n",
      "Epoch 353/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4724 - output_1_loss: 1.5043 - output_2_loss: 0.9684\n",
      "Epoch 354/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4742 - output_1_loss: 1.5043 - output_2_loss: 0.9697\n",
      "Epoch 355/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4736 - output_1_loss: 1.5045 - output_2_loss: 0.9690\n",
      "Epoch 356/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4707 - output_1_loss: 1.5022 - output_2_loss: 0.9684\n",
      "Epoch 357/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4744 - output_1_loss: 1.5056 - output_2_loss: 0.9687\n",
      "Epoch 358/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4736 - output_1_loss: 1.5050 - output_2_loss: 0.9686\n",
      "Epoch 359/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4763 - output_1_loss: 1.5060 - output_2_loss: 0.9702\n",
      "Epoch 360/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4775 - output_1_loss: 1.5086 - output_2_loss: 0.9691\n",
      "Epoch 361/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4759 - output_1_loss: 1.5052 - output_2_loss: 0.9708\n",
      "Epoch 362/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4756 - output_1_loss: 1.5063 - output_2_loss: 0.9692\n",
      "Epoch 363/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4742 - output_1_loss: 1.5048 - output_2_loss: 0.9691\n",
      "Epoch 364/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4774 - output_1_loss: 1.5087 - output_2_loss: 0.9689\n",
      "Epoch 365/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4723 - output_1_loss: 1.5050 - output_2_loss: 0.9674\n",
      "Epoch 366/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4754 - output_1_loss: 1.5052 - output_2_loss: 0.9701\n",
      "Epoch 367/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4737 - output_1_loss: 1.5030 - output_2_loss: 0.9705\n",
      "Epoch 368/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4715 - output_1_loss: 1.5029 - output_2_loss: 0.9684\n",
      "Epoch 369/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4724 - output_1_loss: 1.5036 - output_2_loss: 0.9688\n",
      "Epoch 370/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4701 - output_1_loss: 1.5017 - output_2_loss: 0.9682\n",
      "Epoch 371/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4681 - output_1_loss: 1.4986 - output_2_loss: 0.9696\n",
      "Epoch 372/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4714 - output_1_loss: 1.5020 - output_2_loss: 0.9693\n",
      "Epoch 373/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4727 - output_1_loss: 1.5046 - output_2_loss: 0.9681\n",
      "Epoch 374/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4713 - output_1_loss: 1.5026 - output_2_loss: 0.9684\n",
      "Epoch 375/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4730 - output_1_loss: 1.5039 - output_2_loss: 0.9689\n",
      "Epoch 376/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4733 - output_1_loss: 1.5046 - output_2_loss: 0.9686\n",
      "Epoch 377/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4702 - output_1_loss: 1.5011 - output_2_loss: 0.9692\n",
      "Epoch 378/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4685 - output_1_loss: 1.4995 - output_2_loss: 0.9689\n",
      "Epoch 379/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4737 - output_1_loss: 1.5047 - output_2_loss: 0.9688\n",
      "Epoch 380/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4700 - output_1_loss: 1.5018 - output_2_loss: 0.9681\n",
      "Epoch 381/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4678 - output_1_loss: 1.5002 - output_2_loss: 0.9675\n",
      "Epoch 382/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4695 - output_1_loss: 1.5008 - output_2_loss: 0.9685\n",
      "Epoch 383/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4720 - output_1_loss: 1.5041 - output_2_loss: 0.9681\n",
      "Epoch 384/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4733 - output_1_loss: 1.5044 - output_2_loss: 0.9688\n",
      "Epoch 385/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4720 - output_1_loss: 1.5032 - output_2_loss: 0.9690\n",
      "Epoch 386/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4747 - output_1_loss: 1.5042 - output_2_loss: 0.9704\n",
      "Epoch 387/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4731 - output_1_loss: 1.5035 - output_2_loss: 0.9697\n",
      "Epoch 388/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4706 - output_1_loss: 1.5022 - output_2_loss: 0.9683\n",
      "Epoch 389/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4733 - output_1_loss: 1.5033 - output_2_loss: 0.9705\n",
      "Epoch 390/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4738 - output_1_loss: 1.5053 - output_2_loss: 0.9685\n",
      "Epoch 391/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4676 - output_1_loss: 1.4987 - output_2_loss: 0.9687\n",
      "Epoch 392/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4710 - output_1_loss: 1.5014 - output_2_loss: 0.9695\n",
      "Epoch 393/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4691 - output_1_loss: 1.4996 - output_2_loss: 0.9695\n",
      "Epoch 394/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4682 - output_1_loss: 1.5007 - output_2_loss: 0.9676\n",
      "Epoch 395/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4714 - output_1_loss: 1.5018 - output_2_loss: 0.9698\n",
      "Epoch 396/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4666 - output_1_loss: 1.4984 - output_2_loss: 0.9681\n",
      "Epoch 397/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4728 - output_1_loss: 1.5035 - output_2_loss: 0.9692\n",
      "Epoch 398/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4682 - output_1_loss: 1.4992 - output_2_loss: 0.9689\n",
      "Epoch 399/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4710 - output_1_loss: 1.5018 - output_2_loss: 0.9692\n",
      "Epoch 400/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4716 - output_1_loss: 1.5027 - output_2_loss: 0.9687\n",
      "Epoch 401/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4699 - output_1_loss: 1.5020 - output_2_loss: 0.9680\n",
      "Epoch 402/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4721 - output_1_loss: 1.5037 - output_2_loss: 0.9683\n",
      "Epoch 403/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4725 - output_1_loss: 1.5015 - output_2_loss: 0.9709\n",
      "Epoch 404/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4718 - output_1_loss: 1.5029 - output_2_loss: 0.9687\n",
      "Epoch 405/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4661 - output_1_loss: 1.4978 - output_2_loss: 0.9683\n",
      "Epoch 406/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4700 - output_1_loss: 1.5010 - output_2_loss: 0.9690\n",
      "Epoch 407/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4694 - output_1_loss: 1.5015 - output_2_loss: 0.9679\n",
      "Epoch 408/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4712 - output_1_loss: 1.5016 - output_2_loss: 0.9696\n",
      "Epoch 409/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4692 - output_1_loss: 1.5006 - output_2_loss: 0.9685\n",
      "Epoch 410/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4632 - output_1_loss: 1.4954 - output_2_loss: 0.9680\n",
      "Epoch 411/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4668 - output_1_loss: 1.4986 - output_2_loss: 0.9683\n",
      "Epoch 412/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4682 - output_1_loss: 1.5008 - output_2_loss: 0.9674\n",
      "Epoch 413/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4681 - output_1_loss: 1.5010 - output_2_loss: 0.9672\n",
      "Epoch 414/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4687 - output_1_loss: 1.5012 - output_2_loss: 0.9678\n",
      "Epoch 415/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4677 - output_1_loss: 1.5003 - output_2_loss: 0.9678\n",
      "Epoch 416/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4707 - output_1_loss: 1.5025 - output_2_loss: 0.9684\n",
      "Epoch 417/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4671 - output_1_loss: 1.4995 - output_2_loss: 0.9675\n",
      "Epoch 418/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4647 - output_1_loss: 1.4973 - output_2_loss: 0.9674\n",
      "Epoch 419/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4664 - output_1_loss: 1.4984 - output_2_loss: 0.9679\n",
      "Epoch 420/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4667 - output_1_loss: 1.4971 - output_2_loss: 0.9693\n",
      "Epoch 421/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4657 - output_1_loss: 1.4970 - output_2_loss: 0.9687\n",
      "Epoch 422/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4650 - output_1_loss: 1.4964 - output_2_loss: 0.9685\n",
      "Epoch 423/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4650 - output_1_loss: 1.4955 - output_2_loss: 0.9698\n",
      "Epoch 424/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4659 - output_1_loss: 1.4985 - output_2_loss: 0.9677\n",
      "Epoch 425/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4672 - output_1_loss: 1.4991 - output_2_loss: 0.9682\n",
      "Epoch 426/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4659 - output_1_loss: 1.4956 - output_2_loss: 0.9702\n",
      "Epoch 427/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4667 - output_1_loss: 1.4995 - output_2_loss: 0.9671\n",
      "Epoch 428/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4643 - output_1_loss: 1.4975 - output_2_loss: 0.9670\n",
      "Epoch 429/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4663 - output_1_loss: 1.4983 - output_2_loss: 0.9681\n",
      "Epoch 430/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4682 - output_1_loss: 1.5002 - output_2_loss: 0.9683\n",
      "Epoch 431/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4642 - output_1_loss: 1.4960 - output_2_loss: 0.9681\n",
      "Epoch 432/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4635 - output_1_loss: 1.4954 - output_2_loss: 0.9680\n",
      "Epoch 433/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4653 - output_1_loss: 1.4989 - output_2_loss: 0.9667\n",
      "Epoch 434/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4655 - output_1_loss: 1.4983 - output_2_loss: 0.9670\n",
      "Epoch 435/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4637 - output_1_loss: 1.4959 - output_2_loss: 0.9678\n",
      "Epoch 436/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4678 - output_1_loss: 1.4985 - output_2_loss: 0.9691\n",
      "Epoch 437/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4664 - output_1_loss: 1.4981 - output_2_loss: 0.9682\n",
      "Epoch 438/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4622 - output_1_loss: 1.4946 - output_2_loss: 0.9678\n",
      "Epoch 439/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4623 - output_1_loss: 1.4941 - output_2_loss: 0.9681\n",
      "Epoch 440/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4612 - output_1_loss: 1.4941 - output_2_loss: 0.9673\n",
      "Epoch 441/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4633 - output_1_loss: 1.4955 - output_2_loss: 0.9679\n",
      "Epoch 442/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4627 - output_1_loss: 1.4941 - output_2_loss: 0.9686\n",
      "Epoch 443/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4630 - output_1_loss: 1.4955 - output_2_loss: 0.9676\n",
      "Epoch 444/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4622 - output_1_loss: 1.4934 - output_2_loss: 0.9688\n",
      "Epoch 445/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4616 - output_1_loss: 1.4943 - output_2_loss: 0.9674\n",
      "Epoch 446/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4664 - output_1_loss: 1.4996 - output_2_loss: 0.9668\n",
      "Epoch 447/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4607 - output_1_loss: 1.4932 - output_2_loss: 0.9677\n",
      "Epoch 448/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4661 - output_1_loss: 1.4997 - output_2_loss: 0.9666\n",
      "Epoch 449/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4642 - output_1_loss: 1.4960 - output_2_loss: 0.9682\n",
      "Epoch 450/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4659 - output_1_loss: 1.4999 - output_2_loss: 0.9659\n",
      "Epoch 451/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4592 - output_1_loss: 1.4931 - output_2_loss: 0.9662\n",
      "Epoch 452/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4633 - output_1_loss: 1.4960 - output_2_loss: 0.9672\n",
      "Epoch 453/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4636 - output_1_loss: 1.4954 - output_2_loss: 0.9680\n",
      "Epoch 454/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4611 - output_1_loss: 1.4953 - output_2_loss: 0.9656\n",
      "Epoch 455/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4613 - output_1_loss: 1.4948 - output_2_loss: 0.9668\n",
      "Epoch 456/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4624 - output_1_loss: 1.4954 - output_2_loss: 0.9669\n",
      "Epoch 457/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4614 - output_1_loss: 1.4951 - output_2_loss: 0.9662\n",
      "Epoch 458/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4617 - output_1_loss: 1.4947 - output_2_loss: 0.9669\n",
      "Epoch 459/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4623 - output_1_loss: 1.4955 - output_2_loss: 0.9670\n",
      "Epoch 460/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4617 - output_1_loss: 1.4947 - output_2_loss: 0.9671\n",
      "Epoch 461/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4605 - output_1_loss: 1.4920 - output_2_loss: 0.9684\n",
      "Epoch 462/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4604 - output_1_loss: 1.4944 - output_2_loss: 0.9660\n",
      "Epoch 463/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4578 - output_1_loss: 1.4900 - output_2_loss: 0.9677\n",
      "Epoch 464/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4601 - output_1_loss: 1.4929 - output_2_loss: 0.9674\n",
      "Epoch 465/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4629 - output_1_loss: 1.4960 - output_2_loss: 0.9669\n",
      "Epoch 466/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4615 - output_1_loss: 1.4960 - output_2_loss: 0.9656\n",
      "Epoch 467/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4605 - output_1_loss: 1.4920 - output_2_loss: 0.9683\n",
      "Epoch 468/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4620 - output_1_loss: 1.4948 - output_2_loss: 0.9672\n",
      "Epoch 469/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4572 - output_1_loss: 1.4910 - output_2_loss: 0.9663\n",
      "Epoch 470/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4595 - output_1_loss: 1.4934 - output_2_loss: 0.9660\n",
      "Epoch 471/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4556 - output_1_loss: 1.4888 - output_2_loss: 0.9671\n",
      "Epoch 472/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4592 - output_1_loss: 1.4920 - output_2_loss: 0.9671\n",
      "Epoch 473/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4545 - output_1_loss: 1.4879 - output_2_loss: 0.9664\n",
      "Epoch 474/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4609 - output_1_loss: 1.4940 - output_2_loss: 0.9669\n",
      "Epoch 475/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4577 - output_1_loss: 1.4903 - output_2_loss: 0.9672\n",
      "Epoch 476/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4580 - output_1_loss: 1.4928 - output_2_loss: 0.9651\n",
      "Epoch 477/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4582 - output_1_loss: 1.4920 - output_2_loss: 0.9663\n",
      "Epoch 478/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4560 - output_1_loss: 1.4893 - output_2_loss: 0.9665\n",
      "Epoch 479/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4602 - output_1_loss: 1.4913 - output_2_loss: 0.9689\n",
      "Epoch 480/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4581 - output_1_loss: 1.4904 - output_2_loss: 0.9676\n",
      "Epoch 481/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4553 - output_1_loss: 1.4895 - output_2_loss: 0.9659\n",
      "Epoch 482/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4581 - output_1_loss: 1.4925 - output_2_loss: 0.9658\n",
      "Epoch 483/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4570 - output_1_loss: 1.4903 - output_2_loss: 0.9667\n",
      "Epoch 484/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4572 - output_1_loss: 1.4913 - output_2_loss: 0.9657\n",
      "Epoch 485/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4578 - output_1_loss: 1.4923 - output_2_loss: 0.9653\n",
      "Epoch 486/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4566 - output_1_loss: 1.4912 - output_2_loss: 0.9658\n",
      "Epoch 487/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4565 - output_1_loss: 1.4896 - output_2_loss: 0.9667\n",
      "Epoch 488/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4593 - output_1_loss: 1.4915 - output_2_loss: 0.9677\n",
      "Epoch 489/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4582 - output_1_loss: 1.4928 - output_2_loss: 0.9653\n",
      "Epoch 490/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4548 - output_1_loss: 1.4886 - output_2_loss: 0.9666\n",
      "Epoch 491/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4562 - output_1_loss: 1.4898 - output_2_loss: 0.9662\n",
      "Epoch 492/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4549 - output_1_loss: 1.4877 - output_2_loss: 0.9672\n",
      "Epoch 493/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4558 - output_1_loss: 1.4902 - output_2_loss: 0.9655\n",
      "Epoch 494/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4558 - output_1_loss: 1.4896 - output_2_loss: 0.9665\n",
      "Epoch 495/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4575 - output_1_loss: 1.4909 - output_2_loss: 0.9666\n",
      "Epoch 496/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4555 - output_1_loss: 1.4888 - output_2_loss: 0.9668\n",
      "Epoch 497/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4530 - output_1_loss: 1.4869 - output_2_loss: 0.9660\n",
      "Epoch 498/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4538 - output_1_loss: 1.4874 - output_2_loss: 0.9663\n",
      "Epoch 499/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4558 - output_1_loss: 1.4898 - output_2_loss: 0.9659\n",
      "Epoch 500/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4598 - output_1_loss: 1.4929 - output_2_loss: 0.9668\n",
      "Epoch 501/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4579 - output_1_loss: 1.4923 - output_2_loss: 0.9655\n",
      "Epoch 502/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4516 - output_1_loss: 1.4850 - output_2_loss: 0.9667\n",
      "Epoch 503/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4522 - output_1_loss: 1.4868 - output_2_loss: 0.9655\n",
      "Epoch 504/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4567 - output_1_loss: 1.4911 - output_2_loss: 0.9656\n",
      "Epoch 505/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4537 - output_1_loss: 1.4887 - output_2_loss: 0.9651\n",
      "Epoch 506/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4527 - output_1_loss: 1.4874 - output_2_loss: 0.9651\n",
      "Epoch 507/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4556 - output_1_loss: 1.4899 - output_2_loss: 0.9662\n",
      "Epoch 508/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4528 - output_1_loss: 1.4872 - output_2_loss: 0.9659\n",
      "Epoch 509/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4585 - output_1_loss: 1.4910 - output_2_loss: 0.9676\n",
      "Epoch 510/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4556 - output_1_loss: 1.4899 - output_2_loss: 0.9655\n",
      "Epoch 511/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4516 - output_1_loss: 1.4854 - output_2_loss: 0.9663\n",
      "Epoch 512/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4544 - output_1_loss: 1.4894 - output_2_loss: 0.9649\n",
      "Epoch 513/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4587 - output_1_loss: 1.4925 - output_2_loss: 0.9663\n",
      "Epoch 514/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4572 - output_1_loss: 1.4904 - output_2_loss: 0.9669\n",
      "Epoch 515/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4502 - output_1_loss: 1.4839 - output_2_loss: 0.9664\n",
      "Epoch 516/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4505 - output_1_loss: 1.4857 - output_2_loss: 0.9649\n",
      "Epoch 517/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4551 - output_1_loss: 1.4883 - output_2_loss: 0.9669\n",
      "Epoch 518/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4538 - output_1_loss: 1.4877 - output_2_loss: 0.9658\n",
      "Epoch 519/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4549 - output_1_loss: 1.4895 - output_2_loss: 0.9652\n",
      "Epoch 520/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4495 - output_1_loss: 1.4849 - output_2_loss: 0.9645\n",
      "Epoch 521/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4535 - output_1_loss: 1.4866 - output_2_loss: 0.9668\n",
      "Epoch 522/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4530 - output_1_loss: 1.4886 - output_2_loss: 0.9642\n",
      "Epoch 523/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4526 - output_1_loss: 1.4875 - output_2_loss: 0.9653\n",
      "Epoch 524/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4536 - output_1_loss: 1.4882 - output_2_loss: 0.9651\n",
      "Epoch 525/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4542 - output_1_loss: 1.4884 - output_2_loss: 0.9658\n",
      "Epoch 526/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4540 - output_1_loss: 1.4881 - output_2_loss: 0.9662\n",
      "Epoch 527/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4487 - output_1_loss: 1.4836 - output_2_loss: 0.9651\n",
      "Epoch 528/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4552 - output_1_loss: 1.4884 - output_2_loss: 0.9666\n",
      "Epoch 529/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4505 - output_1_loss: 1.4848 - output_2_loss: 0.9659\n",
      "Epoch 530/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4485 - output_1_loss: 1.4829 - output_2_loss: 0.9657\n",
      "Epoch 531/1000\n",
      "2518/2518 [==============================] - 12s 5ms/step - loss: 2.4512 - output_1_loss: 1.4862 - output_2_loss: 0.9648\n",
      "Epoch 532/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4476 - output_1_loss: 1.4832 - output_2_loss: 0.9644\n",
      "Epoch 533/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4505 - output_1_loss: 1.4853 - output_2_loss: 0.9653\n",
      "Epoch 534/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4492 - output_1_loss: 1.4848 - output_2_loss: 0.9643\n",
      "Epoch 535/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4492 - output_1_loss: 1.4838 - output_2_loss: 0.9652\n",
      "Epoch 536/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4507 - output_1_loss: 1.4855 - output_2_loss: 0.9655\n",
      "Epoch 537/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4522 - output_1_loss: 1.4877 - output_2_loss: 0.9645\n",
      "Epoch 538/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4513 - output_1_loss: 1.4860 - output_2_loss: 0.9656\n",
      "Epoch 539/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4505 - output_1_loss: 1.4840 - output_2_loss: 0.9664\n",
      "Epoch 540/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4507 - output_1_loss: 1.4868 - output_2_loss: 0.9639\n",
      "Epoch 541/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4508 - output_1_loss: 1.4843 - output_2_loss: 0.9665\n",
      "Epoch 542/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4509 - output_1_loss: 1.4858 - output_2_loss: 0.9650\n",
      "Epoch 543/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4469 - output_1_loss: 1.4809 - output_2_loss: 0.9658\n",
      "Epoch 544/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4518 - output_1_loss: 1.4854 - output_2_loss: 0.9663\n",
      "Epoch 545/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4491 - output_1_loss: 1.4838 - output_2_loss: 0.9653\n",
      "Epoch 546/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4523 - output_1_loss: 1.4870 - output_2_loss: 0.9652\n",
      "Epoch 547/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4499 - output_1_loss: 1.4844 - output_2_loss: 0.9653\n",
      "Epoch 548/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4518 - output_1_loss: 1.4865 - output_2_loss: 0.9654\n",
      "Epoch 549/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4503 - output_1_loss: 1.4846 - output_2_loss: 0.9656\n",
      "Epoch 550/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4494 - output_1_loss: 1.4843 - output_2_loss: 0.9650\n",
      "Epoch 551/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4491 - output_1_loss: 1.4843 - output_2_loss: 0.9648\n",
      "Epoch 552/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4474 - output_1_loss: 1.4824 - output_2_loss: 0.9651\n",
      "Epoch 553/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4453 - output_1_loss: 1.4793 - output_2_loss: 0.9660\n",
      "Epoch 554/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4499 - output_1_loss: 1.4842 - output_2_loss: 0.9660\n",
      "Epoch 555/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4492 - output_1_loss: 1.4852 - output_2_loss: 0.9640\n",
      "Epoch 556/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4463 - output_1_loss: 1.4813 - output_2_loss: 0.9649\n",
      "Epoch 557/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4474 - output_1_loss: 1.4814 - output_2_loss: 0.9659\n",
      "Epoch 558/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4470 - output_1_loss: 1.4817 - output_2_loss: 0.9653\n",
      "Epoch 559/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4485 - output_1_loss: 1.4835 - output_2_loss: 0.9649\n",
      "Epoch 560/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4490 - output_1_loss: 1.4825 - output_2_loss: 0.9664\n",
      "Epoch 561/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4504 - output_1_loss: 1.4841 - output_2_loss: 0.9663\n",
      "Epoch 562/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4461 - output_1_loss: 1.4805 - output_2_loss: 0.9658\n",
      "Epoch 563/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4476 - output_1_loss: 1.4827 - output_2_loss: 0.9652\n",
      "Epoch 564/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4465 - output_1_loss: 1.4807 - output_2_loss: 0.9657\n",
      "Epoch 565/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4443 - output_1_loss: 1.4793 - output_2_loss: 0.9648\n",
      "Epoch 566/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4465 - output_1_loss: 1.4811 - output_2_loss: 0.9655\n",
      "Epoch 567/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4497 - output_1_loss: 1.4846 - output_2_loss: 0.9650\n",
      "Epoch 568/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4457 - output_1_loss: 1.4806 - output_2_loss: 0.9653\n",
      "Epoch 569/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4497 - output_1_loss: 1.4848 - output_2_loss: 0.9646\n",
      "Epoch 570/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4478 - output_1_loss: 1.4817 - output_2_loss: 0.9663\n",
      "Epoch 571/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4444 - output_1_loss: 1.4795 - output_2_loss: 0.9650\n",
      "Epoch 572/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4470 - output_1_loss: 1.4815 - output_2_loss: 0.9655\n",
      "Epoch 573/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4447 - output_1_loss: 1.4793 - output_2_loss: 0.9652\n",
      "Epoch 574/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4455 - output_1_loss: 1.4808 - output_2_loss: 0.9648\n",
      "Epoch 575/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4467 - output_1_loss: 1.4814 - output_2_loss: 0.9651\n",
      "Epoch 576/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4444 - output_1_loss: 1.4795 - output_2_loss: 0.9648\n",
      "Epoch 577/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4428 - output_1_loss: 1.4789 - output_2_loss: 0.9637\n",
      "Epoch 578/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4465 - output_1_loss: 1.4810 - output_2_loss: 0.9654\n",
      "Epoch 579/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4459 - output_1_loss: 1.4808 - output_2_loss: 0.9650\n",
      "Epoch 580/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4442 - output_1_loss: 1.4778 - output_2_loss: 0.9663\n",
      "Epoch 581/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4426 - output_1_loss: 1.4779 - output_2_loss: 0.9646\n",
      "Epoch 582/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4461 - output_1_loss: 1.4826 - output_2_loss: 0.9636\n",
      "Epoch 583/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4470 - output_1_loss: 1.4812 - output_2_loss: 0.9659\n",
      "Epoch 584/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4478 - output_1_loss: 1.4825 - output_2_loss: 0.9650\n",
      "Epoch 585/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4462 - output_1_loss: 1.4805 - output_2_loss: 0.9658\n",
      "Epoch 586/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4442 - output_1_loss: 1.4800 - output_2_loss: 0.9644\n",
      "Epoch 587/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4419 - output_1_loss: 1.4779 - output_2_loss: 0.9644\n",
      "Epoch 588/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4431 - output_1_loss: 1.4785 - output_2_loss: 0.9644\n",
      "Epoch 589/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4453 - output_1_loss: 1.4798 - output_2_loss: 0.9656\n",
      "Epoch 590/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4454 - output_1_loss: 1.4816 - output_2_loss: 0.9641\n",
      "Epoch 591/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4444 - output_1_loss: 1.4797 - output_2_loss: 0.9647\n",
      "Epoch 592/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4447 - output_1_loss: 1.4803 - output_2_loss: 0.9643\n",
      "Epoch 593/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4441 - output_1_loss: 1.4788 - output_2_loss: 0.9653\n",
      "Epoch 594/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4417 - output_1_loss: 1.4774 - output_2_loss: 0.9646\n",
      "Epoch 595/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4450 - output_1_loss: 1.4807 - output_2_loss: 0.9644\n",
      "Epoch 596/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4450 - output_1_loss: 1.4808 - output_2_loss: 0.9642\n",
      "Epoch 597/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4455 - output_1_loss: 1.4814 - output_2_loss: 0.9642\n",
      "Epoch 598/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4448 - output_1_loss: 1.4786 - output_2_loss: 0.9663\n",
      "Epoch 599/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4411 - output_1_loss: 1.4762 - output_2_loss: 0.9647\n",
      "Epoch 600/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4461 - output_1_loss: 1.4811 - output_2_loss: 0.9651\n",
      "Epoch 601/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4434 - output_1_loss: 1.4784 - output_2_loss: 0.9650\n",
      "Epoch 602/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4402 - output_1_loss: 1.4764 - output_2_loss: 0.9636\n",
      "Epoch 603/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4424 - output_1_loss: 1.4778 - output_2_loss: 0.9648\n",
      "Epoch 604/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4457 - output_1_loss: 1.4806 - output_2_loss: 0.9651\n",
      "Epoch 605/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4428 - output_1_loss: 1.4779 - output_2_loss: 0.9650\n",
      "Epoch 606/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4407 - output_1_loss: 1.4779 - output_2_loss: 0.9628\n",
      "Epoch 607/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4440 - output_1_loss: 1.4809 - output_2_loss: 0.9630\n",
      "Epoch 608/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4406 - output_1_loss: 1.4761 - output_2_loss: 0.9645\n",
      "Epoch 609/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4448 - output_1_loss: 1.4802 - output_2_loss: 0.9645\n",
      "Epoch 610/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4426 - output_1_loss: 1.4780 - output_2_loss: 0.9644\n",
      "Epoch 611/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4438 - output_1_loss: 1.4800 - output_2_loss: 0.9639\n",
      "Epoch 612/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4458 - output_1_loss: 1.4804 - output_2_loss: 0.9652\n",
      "Epoch 613/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4447 - output_1_loss: 1.4805 - output_2_loss: 0.9641\n",
      "Epoch 614/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4416 - output_1_loss: 1.4771 - output_2_loss: 0.9644\n",
      "Epoch 615/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4413 - output_1_loss: 1.4766 - output_2_loss: 0.9647\n",
      "Epoch 616/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4431 - output_1_loss: 1.4792 - output_2_loss: 0.9643\n",
      "Epoch 617/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4408 - output_1_loss: 1.4773 - output_2_loss: 0.9637\n",
      "Epoch 618/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4432 - output_1_loss: 1.4792 - output_2_loss: 0.9640\n",
      "Epoch 619/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4468 - output_1_loss: 1.4807 - output_2_loss: 0.9658\n",
      "Epoch 620/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4398 - output_1_loss: 1.4742 - output_2_loss: 0.9655\n",
      "Epoch 621/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4431 - output_1_loss: 1.4790 - output_2_loss: 0.9639\n",
      "Epoch 622/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4419 - output_1_loss: 1.4784 - output_2_loss: 0.9634\n",
      "Epoch 623/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4425 - output_1_loss: 1.4772 - output_2_loss: 0.9653\n",
      "Epoch 624/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4429 - output_1_loss: 1.4795 - output_2_loss: 0.9634\n",
      "Epoch 625/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4389 - output_1_loss: 1.4751 - output_2_loss: 0.9639\n",
      "Epoch 626/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4403 - output_1_loss: 1.4772 - output_2_loss: 0.9631\n",
      "Epoch 627/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4431 - output_1_loss: 1.4788 - output_2_loss: 0.9643\n",
      "Epoch 628/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4397 - output_1_loss: 1.4764 - output_2_loss: 0.9639\n",
      "Epoch 629/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4410 - output_1_loss: 1.4753 - output_2_loss: 0.9656\n",
      "Epoch 630/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4385 - output_1_loss: 1.4737 - output_2_loss: 0.9649\n",
      "Epoch 631/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4408 - output_1_loss: 1.4756 - output_2_loss: 0.9652\n",
      "Epoch 632/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4438 - output_1_loss: 1.4795 - output_2_loss: 0.9645\n",
      "Epoch 633/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4398 - output_1_loss: 1.4768 - output_2_loss: 0.9629\n",
      "Epoch 634/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4389 - output_1_loss: 1.4760 - output_2_loss: 0.9629\n",
      "Epoch 635/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4429 - output_1_loss: 1.4773 - output_2_loss: 0.9655\n",
      "Epoch 636/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4422 - output_1_loss: 1.4778 - output_2_loss: 0.9641\n",
      "Epoch 637/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4433 - output_1_loss: 1.4782 - output_2_loss: 0.9655\n",
      "Epoch 638/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4369 - output_1_loss: 1.4722 - output_2_loss: 0.9647\n",
      "Epoch 639/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4398 - output_1_loss: 1.4758 - output_2_loss: 0.9641\n",
      "Epoch 640/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4395 - output_1_loss: 1.4754 - output_2_loss: 0.9642\n",
      "Epoch 641/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4428 - output_1_loss: 1.4786 - output_2_loss: 0.9644\n",
      "Epoch 642/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4422 - output_1_loss: 1.4778 - output_2_loss: 0.9645\n",
      "Epoch 643/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4416 - output_1_loss: 1.4777 - output_2_loss: 0.9639\n",
      "Epoch 644/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4429 - output_1_loss: 1.4786 - output_2_loss: 0.9643\n",
      "Epoch 645/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4438 - output_1_loss: 1.4799 - output_2_loss: 0.9642\n",
      "Epoch 646/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4378 - output_1_loss: 1.4740 - output_2_loss: 0.9638\n",
      "Epoch 647/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4388 - output_1_loss: 1.4737 - output_2_loss: 0.9650\n",
      "Epoch 648/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4371 - output_1_loss: 1.4731 - output_2_loss: 0.9644\n",
      "Epoch 649/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4414 - output_1_loss: 1.4764 - output_2_loss: 0.9647\n",
      "Epoch 650/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4396 - output_1_loss: 1.4757 - output_2_loss: 0.9639\n",
      "Epoch 651/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4369 - output_1_loss: 1.4740 - output_2_loss: 0.9634\n",
      "Epoch 652/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4375 - output_1_loss: 1.4736 - output_2_loss: 0.9639\n",
      "Epoch 653/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4355 - output_1_loss: 1.4714 - output_2_loss: 0.9641\n",
      "Epoch 654/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4360 - output_1_loss: 1.4719 - output_2_loss: 0.9642\n",
      "Epoch 655/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4389 - output_1_loss: 1.4738 - output_2_loss: 0.9652\n",
      "Epoch 656/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4359 - output_1_loss: 1.4740 - output_2_loss: 0.9621\n",
      "Epoch 657/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4402 - output_1_loss: 1.4745 - output_2_loss: 0.9656\n",
      "Epoch 658/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4379 - output_1_loss: 1.4734 - output_2_loss: 0.9645\n",
      "Epoch 659/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4361 - output_1_loss: 1.4729 - output_2_loss: 0.9632\n",
      "Epoch 660/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4393 - output_1_loss: 1.4757 - output_2_loss: 0.9636\n",
      "Epoch 661/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4367 - output_1_loss: 1.4721 - output_2_loss: 0.9645\n",
      "Epoch 662/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4361 - output_1_loss: 1.4709 - output_2_loss: 0.9651\n",
      "Epoch 663/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4411 - output_1_loss: 1.4762 - output_2_loss: 0.9648\n",
      "Epoch 664/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4366 - output_1_loss: 1.4716 - output_2_loss: 0.9650\n",
      "Epoch 665/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4394 - output_1_loss: 1.4744 - output_2_loss: 0.9649\n",
      "Epoch 666/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4393 - output_1_loss: 1.4743 - output_2_loss: 0.9651\n",
      "Epoch 667/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4355 - output_1_loss: 1.4723 - output_2_loss: 0.9630\n",
      "Epoch 668/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4397 - output_1_loss: 1.4752 - output_2_loss: 0.9646\n",
      "Epoch 669/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4334 - output_1_loss: 1.4700 - output_2_loss: 0.9635\n",
      "Epoch 670/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4379 - output_1_loss: 1.4737 - output_2_loss: 0.9639\n",
      "Epoch 671/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4365 - output_1_loss: 1.4729 - output_2_loss: 0.9634\n",
      "Epoch 672/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4359 - output_1_loss: 1.4739 - output_2_loss: 0.9621\n",
      "Epoch 673/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4369 - output_1_loss: 1.4719 - output_2_loss: 0.9651\n",
      "Epoch 674/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4359 - output_1_loss: 1.4725 - output_2_loss: 0.9631\n",
      "Epoch 675/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4388 - output_1_loss: 1.4760 - output_2_loss: 0.9628\n",
      "Epoch 676/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4361 - output_1_loss: 1.4737 - output_2_loss: 0.9624\n",
      "Epoch 677/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4351 - output_1_loss: 1.4713 - output_2_loss: 0.9638\n",
      "Epoch 678/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4355 - output_1_loss: 1.4703 - output_2_loss: 0.9651\n",
      "Epoch 679/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4383 - output_1_loss: 1.4734 - output_2_loss: 0.9648\n",
      "Epoch 680/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4351 - output_1_loss: 1.4716 - output_2_loss: 0.9640\n",
      "Epoch 681/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4364 - output_1_loss: 1.4731 - output_2_loss: 0.9633\n",
      "Epoch 682/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4345 - output_1_loss: 1.4703 - output_2_loss: 0.9641\n",
      "Epoch 683/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4338 - output_1_loss: 1.4696 - output_2_loss: 0.9640\n",
      "Epoch 684/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4380 - output_1_loss: 1.4738 - output_2_loss: 0.9642\n",
      "Epoch 685/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4383 - output_1_loss: 1.4747 - output_2_loss: 0.9635\n",
      "Epoch 686/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4355 - output_1_loss: 1.4721 - output_2_loss: 0.9632\n",
      "Epoch 687/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4323 - output_1_loss: 1.4693 - output_2_loss: 0.9631\n",
      "Epoch 688/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4368 - output_1_loss: 1.4728 - output_2_loss: 0.9647\n",
      "Epoch 689/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4324 - output_1_loss: 1.4703 - output_2_loss: 0.9624\n",
      "Epoch 690/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4410 - output_1_loss: 1.4777 - output_2_loss: 0.9633\n",
      "Epoch 691/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4339 - output_1_loss: 1.4714 - output_2_loss: 0.9626\n",
      "Epoch 692/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4293 - output_1_loss: 1.4663 - output_2_loss: 0.9630\n",
      "Epoch 693/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4346 - output_1_loss: 1.4711 - output_2_loss: 0.9633\n",
      "Epoch 694/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4380 - output_1_loss: 1.4740 - output_2_loss: 0.9641\n",
      "Epoch 695/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4331 - output_1_loss: 1.4685 - output_2_loss: 0.9645\n",
      "Epoch 696/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4345 - output_1_loss: 1.4712 - output_2_loss: 0.9633\n",
      "Epoch 697/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4345 - output_1_loss: 1.4714 - output_2_loss: 0.9628\n",
      "Epoch 698/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4375 - output_1_loss: 1.4737 - output_2_loss: 0.9638\n",
      "Epoch 699/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4342 - output_1_loss: 1.4706 - output_2_loss: 0.9634\n",
      "Epoch 700/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4323 - output_1_loss: 1.4703 - output_2_loss: 0.9620\n",
      "Epoch 701/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4365 - output_1_loss: 1.4726 - output_2_loss: 0.9642\n",
      "Epoch 702/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4312 - output_1_loss: 1.4675 - output_2_loss: 0.9636\n",
      "Epoch 703/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4368 - output_1_loss: 1.4725 - output_2_loss: 0.9643\n",
      "Epoch 704/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4380 - output_1_loss: 1.4739 - output_2_loss: 0.9639\n",
      "Epoch 705/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4323 - output_1_loss: 1.4688 - output_2_loss: 0.9634\n",
      "Epoch 706/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4352 - output_1_loss: 1.4705 - output_2_loss: 0.9646\n",
      "Epoch 707/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4313 - output_1_loss: 1.4677 - output_2_loss: 0.9636\n",
      "Epoch 708/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4365 - output_1_loss: 1.4726 - output_2_loss: 0.9639\n",
      "Epoch 709/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4323 - output_1_loss: 1.4688 - output_2_loss: 0.9634\n",
      "Epoch 710/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4327 - output_1_loss: 1.4690 - output_2_loss: 0.9636\n",
      "Epoch 711/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4320 - output_1_loss: 1.4688 - output_2_loss: 0.9635\n",
      "Epoch 712/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4382 - output_1_loss: 1.4737 - output_2_loss: 0.9645\n",
      "Epoch 713/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4396 - output_1_loss: 1.4752 - output_2_loss: 0.9642\n",
      "Epoch 714/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4358 - output_1_loss: 1.4723 - output_2_loss: 0.9633\n",
      "Epoch 715/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4312 - output_1_loss: 1.4686 - output_2_loss: 0.9625\n",
      "Epoch 716/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4332 - output_1_loss: 1.4700 - output_2_loss: 0.9634\n",
      "Epoch 717/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4332 - output_1_loss: 1.4698 - output_2_loss: 0.9637\n",
      "Epoch 718/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4363 - output_1_loss: 1.4729 - output_2_loss: 0.9634\n",
      "Epoch 719/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4351 - output_1_loss: 1.4707 - output_2_loss: 0.9646\n",
      "Epoch 720/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4346 - output_1_loss: 1.4697 - output_2_loss: 0.9648\n",
      "Epoch 721/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4337 - output_1_loss: 1.4705 - output_2_loss: 0.9631\n",
      "Epoch 722/1000\n",
      "2518/2518 [==============================] - 21s 8ms/step - loss: 2.4347 - output_1_loss: 1.4710 - output_2_loss: 0.9639\n",
      "Epoch 723/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4311 - output_1_loss: 1.4663 - output_2_loss: 0.9647\n",
      "Epoch 724/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4308 - output_1_loss: 1.4667 - output_2_loss: 0.9640\n",
      "Epoch 725/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4344 - output_1_loss: 1.4717 - output_2_loss: 0.9628\n",
      "Epoch 726/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4310 - output_1_loss: 1.4671 - output_2_loss: 0.9638\n",
      "Epoch 727/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4316 - output_1_loss: 1.4677 - output_2_loss: 0.9639\n",
      "Epoch 728/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4330 - output_1_loss: 1.4702 - output_2_loss: 0.9629\n",
      "Epoch 729/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4323 - output_1_loss: 1.4682 - output_2_loss: 0.9642\n",
      "Epoch 730/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4334 - output_1_loss: 1.4711 - output_2_loss: 0.9623\n",
      "Epoch 731/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4334 - output_1_loss: 1.4696 - output_2_loss: 0.9637\n",
      "Epoch 732/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4321 - output_1_loss: 1.4685 - output_2_loss: 0.9639\n",
      "Epoch 733/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4313 - output_1_loss: 1.4679 - output_2_loss: 0.9634\n",
      "Epoch 734/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4331 - output_1_loss: 1.4696 - output_2_loss: 0.9635\n",
      "Epoch 735/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4309 - output_1_loss: 1.4674 - output_2_loss: 0.9637\n",
      "Epoch 736/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4318 - output_1_loss: 1.4684 - output_2_loss: 0.9635\n",
      "Epoch 737/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4311 - output_1_loss: 1.4683 - output_2_loss: 0.9628\n",
      "Epoch 738/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4313 - output_1_loss: 1.4678 - output_2_loss: 0.9637\n",
      "Epoch 739/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4350 - output_1_loss: 1.4724 - output_2_loss: 0.9627\n",
      "Epoch 740/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4333 - output_1_loss: 1.4686 - output_2_loss: 0.9646\n",
      "Epoch 741/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4328 - output_1_loss: 1.4703 - output_2_loss: 0.9625\n",
      "Epoch 742/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4274 - output_1_loss: 1.4656 - output_2_loss: 0.9622\n",
      "Epoch 743/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4313 - output_1_loss: 1.4672 - output_2_loss: 0.9640\n",
      "Epoch 744/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4331 - output_1_loss: 1.4697 - output_2_loss: 0.9632\n",
      "Epoch 745/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4311 - output_1_loss: 1.4678 - output_2_loss: 0.9634\n",
      "Epoch 746/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4279 - output_1_loss: 1.4651 - output_2_loss: 0.9630\n",
      "Epoch 747/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4302 - output_1_loss: 1.4667 - output_2_loss: 0.9633\n",
      "Epoch 748/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4295 - output_1_loss: 1.4659 - output_2_loss: 0.9636\n",
      "Epoch 749/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4332 - output_1_loss: 1.4712 - output_2_loss: 0.9618\n",
      "Epoch 750/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4268 - output_1_loss: 1.4647 - output_2_loss: 0.9622\n",
      "Epoch 751/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4291 - output_1_loss: 1.4659 - output_2_loss: 0.9632\n",
      "Epoch 752/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4288 - output_1_loss: 1.4654 - output_2_loss: 0.9632\n",
      "Epoch 753/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4327 - output_1_loss: 1.4700 - output_2_loss: 0.9628\n",
      "Epoch 754/1000\n",
      "2518/2518 [==============================] - 14s 6ms/step - loss: 2.4326 - output_1_loss: 1.4685 - output_2_loss: 0.9640\n",
      "Epoch 755/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4302 - output_1_loss: 1.4670 - output_2_loss: 0.9630\n",
      "Epoch 756/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4334 - output_1_loss: 1.4707 - output_2_loss: 0.9627\n",
      "Epoch 757/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4308 - output_1_loss: 1.4669 - output_2_loss: 0.9638\n",
      "Epoch 758/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4315 - output_1_loss: 1.4675 - output_2_loss: 0.9640\n",
      "Epoch 759/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4285 - output_1_loss: 1.4661 - output_2_loss: 0.9625\n",
      "Epoch 760/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4306 - output_1_loss: 1.4668 - output_2_loss: 0.9638\n",
      "Epoch 761/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4269 - output_1_loss: 1.4646 - output_2_loss: 0.9621\n",
      "Epoch 762/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4306 - output_1_loss: 1.4661 - output_2_loss: 0.9645\n",
      "Epoch 763/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4263 - output_1_loss: 1.4643 - output_2_loss: 0.9621\n",
      "Epoch 764/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4296 - output_1_loss: 1.4660 - output_2_loss: 0.9635\n",
      "Epoch 765/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4301 - output_1_loss: 1.4661 - output_2_loss: 0.9642\n",
      "Epoch 766/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4297 - output_1_loss: 1.4669 - output_2_loss: 0.9626\n",
      "Epoch 767/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4328 - output_1_loss: 1.4692 - output_2_loss: 0.9635\n",
      "Epoch 768/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4296 - output_1_loss: 1.4657 - output_2_loss: 0.9640\n",
      "Epoch 769/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4321 - output_1_loss: 1.4684 - output_2_loss: 0.9637\n",
      "Epoch 770/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4333 - output_1_loss: 1.4696 - output_2_loss: 0.9635\n",
      "Epoch 771/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4318 - output_1_loss: 1.4675 - output_2_loss: 0.9645\n",
      "Epoch 772/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4293 - output_1_loss: 1.4659 - output_2_loss: 0.9633\n",
      "Epoch 773/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4326 - output_1_loss: 1.4698 - output_2_loss: 0.9629\n",
      "Epoch 774/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4313 - output_1_loss: 1.4681 - output_2_loss: 0.9631\n",
      "Epoch 775/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4337 - output_1_loss: 1.4697 - output_2_loss: 0.9638\n",
      "Epoch 776/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4290 - output_1_loss: 1.4652 - output_2_loss: 0.9636\n",
      "Epoch 777/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4293 - output_1_loss: 1.4660 - output_2_loss: 0.9634\n",
      "Epoch 778/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4259 - output_1_loss: 1.4626 - output_2_loss: 0.9632\n",
      "Epoch 779/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4282 - output_1_loss: 1.4646 - output_2_loss: 0.9637\n",
      "Epoch 780/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4297 - output_1_loss: 1.4661 - output_2_loss: 0.9637\n",
      "Epoch 781/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4263 - output_1_loss: 1.4628 - output_2_loss: 0.9633\n",
      "Epoch 782/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4280 - output_1_loss: 1.4655 - output_2_loss: 0.9624\n",
      "Epoch 783/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4279 - output_1_loss: 1.4651 - output_2_loss: 0.9627\n",
      "Epoch 784/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4291 - output_1_loss: 1.4663 - output_2_loss: 0.9633\n",
      "Epoch 785/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4301 - output_1_loss: 1.4660 - output_2_loss: 0.9639\n",
      "Epoch 786/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4307 - output_1_loss: 1.4676 - output_2_loss: 0.9631\n",
      "Epoch 787/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4270 - output_1_loss: 1.4656 - output_2_loss: 0.9616\n",
      "Epoch 788/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4290 - output_1_loss: 1.4663 - output_2_loss: 0.9629\n",
      "Epoch 789/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4278 - output_1_loss: 1.4635 - output_2_loss: 0.9643\n",
      "Epoch 790/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4300 - output_1_loss: 1.4669 - output_2_loss: 0.9633\n",
      "Epoch 791/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4284 - output_1_loss: 1.4648 - output_2_loss: 0.9635\n",
      "Epoch 792/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4311 - output_1_loss: 1.4674 - output_2_loss: 0.9636\n",
      "Epoch 793/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4295 - output_1_loss: 1.4640 - output_2_loss: 0.9653\n",
      "Epoch 794/1000\n",
      "2518/2518 [==============================] - 16s 7ms/step - loss: 2.4239 - output_1_loss: 1.4613 - output_2_loss: 0.9628\n",
      "Epoch 795/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4275 - output_1_loss: 1.4646 - output_2_loss: 0.9629\n",
      "Epoch 796/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4306 - output_1_loss: 1.4670 - output_2_loss: 0.9636\n",
      "Epoch 797/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4294 - output_1_loss: 1.4663 - output_2_loss: 0.9631\n",
      "Epoch 798/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4266 - output_1_loss: 1.4632 - output_2_loss: 0.9633\n",
      "Epoch 799/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4297 - output_1_loss: 1.4666 - output_2_loss: 0.9632\n",
      "Epoch 800/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4280 - output_1_loss: 1.4656 - output_2_loss: 0.9625\n",
      "Epoch 801/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4284 - output_1_loss: 1.4649 - output_2_loss: 0.9633\n",
      "Epoch 802/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4300 - output_1_loss: 1.4671 - output_2_loss: 0.9630\n",
      "Epoch 803/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4267 - output_1_loss: 1.4640 - output_2_loss: 0.9631\n",
      "Epoch 804/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4268 - output_1_loss: 1.4635 - output_2_loss: 0.9632\n",
      "Epoch 805/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4276 - output_1_loss: 1.4650 - output_2_loss: 0.9629\n",
      "Epoch 806/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4277 - output_1_loss: 1.4646 - output_2_loss: 0.9632\n",
      "Epoch 807/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4258 - output_1_loss: 1.4633 - output_2_loss: 0.9622\n",
      "Epoch 808/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4309 - output_1_loss: 1.4659 - output_2_loss: 0.9651\n",
      "Epoch 809/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4284 - output_1_loss: 1.4646 - output_2_loss: 0.9637\n",
      "Epoch 810/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4281 - output_1_loss: 1.4647 - output_2_loss: 0.9633\n",
      "Epoch 811/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4250 - output_1_loss: 1.4632 - output_2_loss: 0.9620\n",
      "Epoch 812/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4284 - output_1_loss: 1.4656 - output_2_loss: 0.9629\n",
      "Epoch 813/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4312 - output_1_loss: 1.4675 - output_2_loss: 0.9636\n",
      "Epoch 814/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4271 - output_1_loss: 1.4651 - output_2_loss: 0.9622\n",
      "Epoch 815/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4256 - output_1_loss: 1.4618 - output_2_loss: 0.9637\n",
      "Epoch 816/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4294 - output_1_loss: 1.4661 - output_2_loss: 0.9634\n",
      "Epoch 817/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4254 - output_1_loss: 1.4628 - output_2_loss: 0.9629\n",
      "Epoch 818/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4268 - output_1_loss: 1.4633 - output_2_loss: 0.9634\n",
      "Epoch 819/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4254 - output_1_loss: 1.4626 - output_2_loss: 0.9627\n",
      "Epoch 820/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4277 - output_1_loss: 1.4647 - output_2_loss: 0.9630\n",
      "Epoch 821/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4303 - output_1_loss: 1.4662 - output_2_loss: 0.9640\n",
      "Epoch 822/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4260 - output_1_loss: 1.4635 - output_2_loss: 0.9624\n",
      "Epoch 823/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4224 - output_1_loss: 1.4593 - output_2_loss: 0.9631\n",
      "Epoch 824/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4209 - output_1_loss: 1.4596 - output_2_loss: 0.9613\n",
      "Epoch 825/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4237 - output_1_loss: 1.4605 - output_2_loss: 0.9634\n",
      "Epoch 826/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4231 - output_1_loss: 1.4600 - output_2_loss: 0.9630\n",
      "Epoch 827/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4269 - output_1_loss: 1.4625 - output_2_loss: 0.9641\n",
      "Epoch 828/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4233 - output_1_loss: 1.4603 - output_2_loss: 0.9631\n",
      "Epoch 829/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4250 - output_1_loss: 1.4619 - output_2_loss: 0.9629\n",
      "Epoch 830/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4238 - output_1_loss: 1.4596 - output_2_loss: 0.9639\n",
      "Epoch 831/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4263 - output_1_loss: 1.4618 - output_2_loss: 0.9644\n",
      "Epoch 832/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4312 - output_1_loss: 1.4676 - output_2_loss: 0.9637\n",
      "Epoch 833/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4248 - output_1_loss: 1.4596 - output_2_loss: 0.9650\n",
      "Epoch 834/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4262 - output_1_loss: 1.4620 - output_2_loss: 0.9641\n",
      "Epoch 835/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4228 - output_1_loss: 1.4606 - output_2_loss: 0.9621\n",
      "Epoch 836/1000\n",
      "2518/2518 [==============================] - 19s 8ms/step - loss: 2.4255 - output_1_loss: 1.4628 - output_2_loss: 0.9628\n",
      "Epoch 837/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4245 - output_1_loss: 1.4615 - output_2_loss: 0.9632\n",
      "Epoch 838/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4223 - output_1_loss: 1.4594 - output_2_loss: 0.9629\n",
      "Epoch 839/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4279 - output_1_loss: 1.4639 - output_2_loss: 0.9639\n",
      "Epoch 840/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4254 - output_1_loss: 1.4622 - output_2_loss: 0.9630\n",
      "Epoch 841/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4252 - output_1_loss: 1.4635 - output_2_loss: 0.9620\n",
      "Epoch 842/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4222 - output_1_loss: 1.4602 - output_2_loss: 0.9620\n",
      "Epoch 843/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4227 - output_1_loss: 1.4595 - output_2_loss: 0.9631\n",
      "Epoch 844/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4235 - output_1_loss: 1.4607 - output_2_loss: 0.9628\n",
      "Epoch 845/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4257 - output_1_loss: 1.4632 - output_2_loss: 0.9626\n",
      "Epoch 846/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4279 - output_1_loss: 1.4650 - output_2_loss: 0.9629\n",
      "Epoch 847/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4253 - output_1_loss: 1.4623 - output_2_loss: 0.9631\n",
      "Epoch 848/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4253 - output_1_loss: 1.4619 - output_2_loss: 0.9635\n",
      "Epoch 849/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4241 - output_1_loss: 1.4608 - output_2_loss: 0.9632\n",
      "Epoch 850/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4244 - output_1_loss: 1.4607 - output_2_loss: 0.9637\n",
      "Epoch 851/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4218 - output_1_loss: 1.4597 - output_2_loss: 0.9620\n",
      "Epoch 852/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4277 - output_1_loss: 1.4637 - output_2_loss: 0.9638\n",
      "Epoch 853/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4216 - output_1_loss: 1.4599 - output_2_loss: 0.9615\n",
      "Epoch 854/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4230 - output_1_loss: 1.4611 - output_2_loss: 0.9619\n",
      "Epoch 855/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4215 - output_1_loss: 1.4587 - output_2_loss: 0.9626\n",
      "Epoch 856/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4249 - output_1_loss: 1.4623 - output_2_loss: 0.9626\n",
      "Epoch 857/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4241 - output_1_loss: 1.4614 - output_2_loss: 0.9629\n",
      "Epoch 858/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4188 - output_1_loss: 1.4567 - output_2_loss: 0.9623\n",
      "Epoch 859/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4225 - output_1_loss: 1.4602 - output_2_loss: 0.9623\n",
      "Epoch 860/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4206 - output_1_loss: 1.4586 - output_2_loss: 0.9619\n",
      "Epoch 861/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4229 - output_1_loss: 1.4595 - output_2_loss: 0.9632\n",
      "Epoch 862/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4203 - output_1_loss: 1.4569 - output_2_loss: 0.9633\n",
      "Epoch 863/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4235 - output_1_loss: 1.4597 - output_2_loss: 0.9639\n",
      "Epoch 864/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4240 - output_1_loss: 1.4610 - output_2_loss: 0.9628\n",
      "Epoch 865/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4242 - output_1_loss: 1.4618 - output_2_loss: 0.9624\n",
      "Epoch 866/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4257 - output_1_loss: 1.4634 - output_2_loss: 0.9623\n",
      "Epoch 867/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4216 - output_1_loss: 1.4590 - output_2_loss: 0.9624\n",
      "Epoch 868/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4251 - output_1_loss: 1.4617 - output_2_loss: 0.9633\n",
      "Epoch 869/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4231 - output_1_loss: 1.4600 - output_2_loss: 0.9631\n",
      "Epoch 870/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4227 - output_1_loss: 1.4607 - output_2_loss: 0.9618\n",
      "Epoch 871/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4221 - output_1_loss: 1.4586 - output_2_loss: 0.9633\n",
      "Epoch 872/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4200 - output_1_loss: 1.4560 - output_2_loss: 0.9640\n",
      "Epoch 873/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4247 - output_1_loss: 1.4619 - output_2_loss: 0.9626\n",
      "Epoch 874/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4192 - output_1_loss: 1.4575 - output_2_loss: 0.9617\n",
      "Epoch 875/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4210 - output_1_loss: 1.4582 - output_2_loss: 0.9625\n",
      "Epoch 876/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4206 - output_1_loss: 1.4573 - output_2_loss: 0.9632\n",
      "Epoch 877/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4209 - output_1_loss: 1.4586 - output_2_loss: 0.9623\n",
      "Epoch 878/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4236 - output_1_loss: 1.4614 - output_2_loss: 0.9622\n",
      "Epoch 879/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4184 - output_1_loss: 1.4560 - output_2_loss: 0.9626\n",
      "Epoch 880/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4261 - output_1_loss: 1.4627 - output_2_loss: 0.9633\n",
      "Epoch 881/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4205 - output_1_loss: 1.4579 - output_2_loss: 0.9627\n",
      "Epoch 882/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4192 - output_1_loss: 1.4556 - output_2_loss: 0.9634\n",
      "Epoch 883/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4203 - output_1_loss: 1.4573 - output_2_loss: 0.9629\n",
      "Epoch 884/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4196 - output_1_loss: 1.4567 - output_2_loss: 0.9628\n",
      "Epoch 885/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4198 - output_1_loss: 1.4576 - output_2_loss: 0.9625\n",
      "Epoch 886/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4204 - output_1_loss: 1.4581 - output_2_loss: 0.9624\n",
      "Epoch 887/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4235 - output_1_loss: 1.4610 - output_2_loss: 0.9625\n",
      "Epoch 888/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4237 - output_1_loss: 1.4609 - output_2_loss: 0.9628\n",
      "Epoch 889/1000\n",
      "2518/2518 [==============================] - 21s 8ms/step - loss: 2.4207 - output_1_loss: 1.4582 - output_2_loss: 0.9625\n",
      "Epoch 890/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4176 - output_1_loss: 1.4548 - output_2_loss: 0.9629\n",
      "Epoch 891/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4249 - output_1_loss: 1.4622 - output_2_loss: 0.9627\n",
      "Epoch 892/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4216 - output_1_loss: 1.4598 - output_2_loss: 0.9619\n",
      "Epoch 893/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4236 - output_1_loss: 1.4612 - output_2_loss: 0.9624\n",
      "Epoch 894/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4226 - output_1_loss: 1.4594 - output_2_loss: 0.9631\n",
      "Epoch 895/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4223 - output_1_loss: 1.4588 - output_2_loss: 0.9634\n",
      "Epoch 896/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4224 - output_1_loss: 1.4592 - output_2_loss: 0.9633\n",
      "Epoch 897/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4218 - output_1_loss: 1.4585 - output_2_loss: 0.9633\n",
      "Epoch 898/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4206 - output_1_loss: 1.4578 - output_2_loss: 0.9626\n",
      "Epoch 899/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4217 - output_1_loss: 1.4583 - output_2_loss: 0.9633\n",
      "Epoch 900/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4173 - output_1_loss: 1.4544 - output_2_loss: 0.9629\n",
      "Epoch 901/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4193 - output_1_loss: 1.4570 - output_2_loss: 0.9621\n",
      "Epoch 902/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4195 - output_1_loss: 1.4574 - output_2_loss: 0.9622\n",
      "Epoch 903/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4218 - output_1_loss: 1.4583 - output_2_loss: 0.9636\n",
      "Epoch 904/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4192 - output_1_loss: 1.4569 - output_2_loss: 0.9624\n",
      "Epoch 905/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4179 - output_1_loss: 1.4557 - output_2_loss: 0.9620\n",
      "Epoch 906/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4234 - output_1_loss: 1.4597 - output_2_loss: 0.9637\n",
      "Epoch 907/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4178 - output_1_loss: 1.4562 - output_2_loss: 0.9616\n",
      "Epoch 908/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4234 - output_1_loss: 1.4614 - output_2_loss: 0.9621\n",
      "Epoch 909/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4237 - output_1_loss: 1.4607 - output_2_loss: 0.9630\n",
      "Epoch 910/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4204 - output_1_loss: 1.4588 - output_2_loss: 0.9618\n",
      "Epoch 911/1000\n",
      "2518/2518 [==============================] - 16s 6ms/step - loss: 2.4239 - output_1_loss: 1.4614 - output_2_loss: 0.9623\n",
      "Epoch 912/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4200 - output_1_loss: 1.4568 - output_2_loss: 0.9635\n",
      "Epoch 913/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4186 - output_1_loss: 1.4555 - output_2_loss: 0.9630\n",
      "Epoch 914/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4208 - output_1_loss: 1.4576 - output_2_loss: 0.9633\n",
      "Epoch 915/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4212 - output_1_loss: 1.4588 - output_2_loss: 0.9624\n",
      "Epoch 916/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4247 - output_1_loss: 1.4614 - output_2_loss: 0.9632\n",
      "Epoch 917/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4201 - output_1_loss: 1.4591 - output_2_loss: 0.9611\n",
      "Epoch 918/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4168 - output_1_loss: 1.4541 - output_2_loss: 0.9626\n",
      "Epoch 919/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4216 - output_1_loss: 1.4584 - output_2_loss: 0.9630\n",
      "Epoch 920/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4213 - output_1_loss: 1.4591 - output_2_loss: 0.9626\n",
      "Epoch 921/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4189 - output_1_loss: 1.4566 - output_2_loss: 0.9624\n",
      "Epoch 922/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4214 - output_1_loss: 1.4589 - output_2_loss: 0.9624\n",
      "Epoch 923/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4178 - output_1_loss: 1.4553 - output_2_loss: 0.9623\n",
      "Epoch 924/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4213 - output_1_loss: 1.4574 - output_2_loss: 0.9640\n",
      "Epoch 925/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4189 - output_1_loss: 1.4562 - output_2_loss: 0.9625\n",
      "Epoch 926/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4197 - output_1_loss: 1.4571 - output_2_loss: 0.9627\n",
      "Epoch 927/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4167 - output_1_loss: 1.4541 - output_2_loss: 0.9624\n",
      "Epoch 928/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4170 - output_1_loss: 1.4550 - output_2_loss: 0.9619\n",
      "Epoch 929/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4189 - output_1_loss: 1.4574 - output_2_loss: 0.9616\n",
      "Epoch 930/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4195 - output_1_loss: 1.4571 - output_2_loss: 0.9623\n",
      "Epoch 931/1000\n",
      "2518/2518 [==============================] - 15s 6ms/step - loss: 2.4225 - output_1_loss: 1.4600 - output_2_loss: 0.9626\n",
      "Epoch 932/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4209 - output_1_loss: 1.4586 - output_2_loss: 0.9622\n",
      "Epoch 933/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4177 - output_1_loss: 1.4558 - output_2_loss: 0.9620\n",
      "Epoch 934/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4190 - output_1_loss: 1.4563 - output_2_loss: 0.9628\n",
      "Epoch 935/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4206 - output_1_loss: 1.4576 - output_2_loss: 0.9631\n",
      "Epoch 936/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4187 - output_1_loss: 1.4566 - output_2_loss: 0.9622\n",
      "Epoch 937/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4183 - output_1_loss: 1.4557 - output_2_loss: 0.9626\n",
      "Epoch 938/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4189 - output_1_loss: 1.4572 - output_2_loss: 0.9616\n",
      "Epoch 939/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4204 - output_1_loss: 1.4574 - output_2_loss: 0.9630\n",
      "Epoch 940/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4222 - output_1_loss: 1.4598 - output_2_loss: 0.9624\n",
      "Epoch 941/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4186 - output_1_loss: 1.4546 - output_2_loss: 0.9642\n",
      "Epoch 942/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4205 - output_1_loss: 1.4572 - output_2_loss: 0.9632\n",
      "Epoch 943/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4213 - output_1_loss: 1.4578 - output_2_loss: 0.9634\n",
      "Epoch 944/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4198 - output_1_loss: 1.4559 - output_2_loss: 0.9640\n",
      "Epoch 945/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4199 - output_1_loss: 1.4575 - output_2_loss: 0.9621\n",
      "Epoch 946/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4204 - output_1_loss: 1.4575 - output_2_loss: 0.9630\n",
      "Epoch 947/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4189 - output_1_loss: 1.4562 - output_2_loss: 0.9628\n",
      "Epoch 948/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4214 - output_1_loss: 1.4578 - output_2_loss: 0.9638\n",
      "Epoch 949/1000\n",
      "2518/2518 [==============================] - 17s 7ms/step - loss: 2.4168 - output_1_loss: 1.4548 - output_2_loss: 0.9618\n",
      "Epoch 950/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4189 - output_1_loss: 1.4569 - output_2_loss: 0.9620\n",
      "Epoch 951/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4196 - output_1_loss: 1.4562 - output_2_loss: 0.9634\n",
      "Epoch 952/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4160 - output_1_loss: 1.4539 - output_2_loss: 0.9622\n",
      "Epoch 953/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4180 - output_1_loss: 1.4553 - output_2_loss: 0.9625\n",
      "Epoch 954/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4171 - output_1_loss: 1.4539 - output_2_loss: 0.9634\n",
      "Epoch 955/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4181 - output_1_loss: 1.4560 - output_2_loss: 0.9623\n",
      "Epoch 956/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4159 - output_1_loss: 1.4533 - output_2_loss: 0.9624\n",
      "Epoch 957/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4171 - output_1_loss: 1.4542 - output_2_loss: 0.9629\n",
      "Epoch 958/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4170 - output_1_loss: 1.4546 - output_2_loss: 0.9625\n",
      "Epoch 959/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4170 - output_1_loss: 1.4557 - output_2_loss: 0.9616\n",
      "Epoch 960/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4168 - output_1_loss: 1.4539 - output_2_loss: 0.9628\n",
      "Epoch 961/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4156 - output_1_loss: 1.4529 - output_2_loss: 0.9627\n",
      "Epoch 962/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4210 - output_1_loss: 1.4579 - output_2_loss: 0.9630\n",
      "Epoch 963/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4159 - output_1_loss: 1.4524 - output_2_loss: 0.9633\n",
      "Epoch 964/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4183 - output_1_loss: 1.4563 - output_2_loss: 0.9619\n",
      "Epoch 965/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4187 - output_1_loss: 1.4549 - output_2_loss: 0.9637\n",
      "Epoch 966/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4157 - output_1_loss: 1.4527 - output_2_loss: 0.9632\n",
      "Epoch 967/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4187 - output_1_loss: 1.4557 - output_2_loss: 0.9629\n",
      "Epoch 968/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4174 - output_1_loss: 1.4549 - output_2_loss: 0.9624\n",
      "Epoch 969/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4166 - output_1_loss: 1.4541 - output_2_loss: 0.9626\n",
      "Epoch 970/1000\n",
      "2518/2518 [==============================] - 19s 7ms/step - loss: 2.4176 - output_1_loss: 1.4545 - output_2_loss: 0.9630\n",
      "Epoch 971/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4150 - output_1_loss: 1.4535 - output_2_loss: 0.9614\n",
      "Epoch 972/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4141 - output_1_loss: 1.4521 - output_2_loss: 0.9622\n",
      "Epoch 973/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4167 - output_1_loss: 1.4546 - output_2_loss: 0.9620\n",
      "Epoch 974/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4165 - output_1_loss: 1.4536 - output_2_loss: 0.9627\n",
      "Epoch 975/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4197 - output_1_loss: 1.4575 - output_2_loss: 0.9622\n",
      "Epoch 976/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4178 - output_1_loss: 1.4570 - output_2_loss: 0.9608\n",
      "Epoch 977/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4183 - output_1_loss: 1.4557 - output_2_loss: 0.9627\n",
      "Epoch 978/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4161 - output_1_loss: 1.4543 - output_2_loss: 0.9615\n",
      "Epoch 979/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4228 - output_1_loss: 1.4594 - output_2_loss: 0.9632\n",
      "Epoch 980/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4154 - output_1_loss: 1.4533 - output_2_loss: 0.9622\n",
      "Epoch 981/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4143 - output_1_loss: 1.4525 - output_2_loss: 0.9617\n",
      "Epoch 982/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4166 - output_1_loss: 1.4533 - output_2_loss: 0.9633\n",
      "Epoch 983/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4192 - output_1_loss: 1.4554 - output_2_loss: 0.9636\n",
      "Epoch 984/1000\n",
      "2518/2518 [==============================] - 14s 5ms/step - loss: 2.4136 - output_1_loss: 1.4523 - output_2_loss: 0.9611\n",
      "Epoch 985/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4137 - output_1_loss: 1.4513 - output_2_loss: 0.9625\n",
      "Epoch 986/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4156 - output_1_loss: 1.4533 - output_2_loss: 0.9623\n",
      "Epoch 987/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4160 - output_1_loss: 1.4539 - output_2_loss: 0.9624\n",
      "Epoch 988/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4158 - output_1_loss: 1.4526 - output_2_loss: 0.9630\n",
      "Epoch 989/1000\n",
      "2518/2518 [==============================] - 20s 8ms/step - loss: 2.4124 - output_1_loss: 1.4507 - output_2_loss: 0.9619\n",
      "Epoch 990/1000\n",
      "2518/2518 [==============================] - 18s 7ms/step - loss: 2.4173 - output_1_loss: 1.4557 - output_2_loss: 0.9617\n",
      "Epoch 991/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4185 - output_1_loss: 1.4562 - output_2_loss: 0.9622\n",
      "Epoch 992/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4125 - output_1_loss: 1.4513 - output_2_loss: 0.9612\n",
      "Epoch 993/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4151 - output_1_loss: 1.4531 - output_2_loss: 0.9620\n",
      "Epoch 994/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4176 - output_1_loss: 1.4551 - output_2_loss: 0.9624\n",
      "Epoch 995/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4131 - output_1_loss: 1.4502 - output_2_loss: 0.9628\n",
      "Epoch 996/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4133 - output_1_loss: 1.4506 - output_2_loss: 0.9626\n",
      "Epoch 997/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4180 - output_1_loss: 1.4548 - output_2_loss: 0.9631\n",
      "Epoch 998/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4157 - output_1_loss: 1.4530 - output_2_loss: 0.9626\n",
      "Epoch 999/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4164 - output_1_loss: 1.4531 - output_2_loss: 0.9631\n",
      "Epoch 1000/1000\n",
      "2518/2518 [==============================] - 13s 5ms/step - loss: 2.4144 - output_1_loss: 1.4532 - output_2_loss: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fb7e607d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# design a tensorflow keras classfication model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# train_input = np.array(train_input)\n",
    "# train_output = np.array(train_output)\n",
    "\n",
    "# print(train_input.shape)\n",
    "# print(train_output.shape)\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(128, activation='relu', input_shape=[101]),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    input_layer = layers.Input(shape=(100,))\n",
    "    X = layers.Dense(128, activation='relu')(input_layer)\n",
    "#     X = layers.Dropout(0.5)(X)\n",
    "    X = layers.Dense(64, activation='relu')(X)\n",
    "#     X = layers.Dropout(0.5)(X)\n",
    "    output_layer_1 = layers.Dense(7,name=\"output_1\")(X)\n",
    "    output_layer_2 = layers.Dense(3,name=\"output_2\")(output_layer_1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_layer], outputs=[output_layer_1,output_layer_2])\n",
    "    return model\n",
    "model = build_model()\n",
    "    \n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy'])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\"output_1\":loss1, \"output_2\": loss2},\n",
    "    metrics={\"output_1\":loss1,\"output_2\":loss2}\n",
    ")\n",
    "train_output = (train_output_1, train_output_2)\n",
    "model.fit(train_input, train_output, epochs=1000, batch_size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89ab3f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T13:53:17.164891Z",
     "iopub.status.busy": "2022-11-13T13:53:17.164474Z",
     "iopub.status.idle": "2022-11-13T13:53:19.592117Z",
     "shell.execute_reply": "2022-11-13T13:53:19.591180Z"
    },
    "papermill": {
     "duration": 18.450978,
     "end_time": "2022-11-13T13:53:19.594906",
     "exception": false,
     "start_time": "2022-11-13T13:53:01.143928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-bert/bert-base-uncased were not used when initializing DistilBertModel: ['bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'cls.predictions.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at ../input/huggingface-bert/bert-base-uncased and are newly initialized: ['transformer.layer.6.attention.k_lin.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.9.ffn.lin1.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.q_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create a submissions.csv file\n",
    "# '''\n",
    "# discourse_id,Ineffective,Adequate,Effective\n",
    "# a261b6e14276,0.2,0.6,0.4\n",
    "# 5a88900e7dc1,3.0,6.0,1.0\n",
    "# 9790d835736b,1.0,2.0,3.0\n",
    "# 75ce6d68b67b,0.33,0.34,0.33\n",
    "# 93578d946723,0.01,0.24,0.47\n",
    "# 2e214524dbe3,0.2,0.6,0.4\n",
    "# 84812fc2ab9f,3.0,6.0,1.0\n",
    "# c668ff840720,1.0,2.0,3.0\n",
    "# 739a6d00f44a,0.33,0.34,0.33\n",
    "# bcfae2c9a244,0.01,0.24,0.47\n",
    "# '''\n",
    "\n",
    "test_data = pd.read_csv('../input/feedback-prize-effectiveness/test.csv')\n",
    "test_data.head()\n",
    "\n",
    "test_sentences = test_data['discourse_text'].tolist()\n",
    "# test_sentences[0]\n",
    "\n",
    "test_dis_types  = test_data['discourse_type'].tolist()\n",
    "\n",
    "test_input,_,_ = getVectors(test_sentences,test_dis_types)\n",
    "test_input = np.array(test_input)\n",
    "\n",
    "test_output = model.predict(test_input)\n",
    "# test_output = np.argmax(test_output, axis=1)\n",
    "# print(test_output)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('submission.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"])\n",
    "    for i in range(len(test_output[1])):\n",
    "#         temp = [0,0,0]\n",
    "#         temp[test_output[i]] = 1\n",
    "        writer.writerow([test_data['discourse_id'][i],test_output[1][i][0],test_output[1][i][1],test_output[1][i][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6de05462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T13:53:51.150005Z",
     "iopub.status.busy": "2022-11-13T13:53:51.149650Z",
     "iopub.status.idle": "2022-11-13T13:53:51.153867Z",
     "shell.execute_reply": "2022-11-13T13:53:51.152895Z"
    },
    "papermill": {
     "duration": 16.155732,
     "end_time": "2022-11-13T13:53:51.156101",
     "exception": false,
     "start_time": "2022-11-13T13:53:35.000369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_output[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cea90fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T13:54:22.174011Z",
     "iopub.status.busy": "2022-11-13T13:54:22.173648Z",
     "iopub.status.idle": "2022-11-13T13:54:22.178842Z",
     "shell.execute_reply": "2022-11-13T13:54:22.177737Z"
    },
    "papermill": {
     "duration": 15.629204,
     "end_time": "2022-11-13T13:54:22.182469",
     "exception": false,
     "start_time": "2022-11-13T13:54:06.553265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get accuracy of the model\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(train_output, model.predict(train_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49ca03",
   "metadata": {
    "papermill": {
     "duration": 16.04606,
     "end_time": "2022-11-13T13:54:53.731980",
     "exception": false,
     "start_time": "2022-11-13T13:54:37.685920",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11b17e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T13:55:24.990491Z",
     "iopub.status.busy": "2022-11-13T13:55:24.990124Z",
     "iopub.status.idle": "2022-11-13T13:55:24.994321Z",
     "shell.execute_reply": "2022-11-13T13:55:24.993310Z"
    },
    "papermill": {
     "duration": 15.864705,
     "end_time": "2022-11-13T13:55:24.996566",
     "exception": false,
     "start_time": "2022-11-13T13:55:09.131861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c70376d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T13:55:56.281969Z",
     "iopub.status.busy": "2022-11-13T13:55:56.281603Z",
     "iopub.status.idle": "2022-11-13T13:55:56.286136Z",
     "shell.execute_reply": "2022-11-13T13:55:56.285215Z"
    },
    "papermill": {
     "duration": 15.716152,
     "end_time": "2022-11-13T13:55:56.288333",
     "exception": false,
     "start_time": "2022-11-13T13:55:40.572181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define losses according to respective class weights\n",
    "#build model with two output layers\n",
    "#redifine get vectors, it will now return two output matrices\n",
    "#compil model\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0456384",
   "metadata": {
    "papermill": {
     "duration": 15.783639,
     "end_time": "2022-11-13T13:56:27.485825",
     "exception": false,
     "start_time": "2022-11-13T13:56:11.702186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14462.182311,
   "end_time": "2022-11-13T13:56:46.645141",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-13T09:55:44.462830",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
