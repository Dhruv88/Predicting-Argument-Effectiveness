{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\n# nltk.download('punkt')\n# from nltk.tokenize import word_tokenize\nimport numpy as np","metadata":{"id":"kb1dt1Bjt9NA","outputId":"083d6bb7-751f-47d3-9cc1-4f24d0a81a70","execution":{"iopub.status.busy":"2022-11-17T14:25:02.193033Z","iopub.execute_input":"2022-11-17T14:25:02.193479Z","iopub.status.idle":"2022-11-17T14:25:02.199676Z","shell.execute_reply.started":"2022-11-17T14:25:02.193445Z","shell.execute_reply":"2022-11-17T14:25:02.198089Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sentences = [\"I ate dinner.\", \"We had a three-course meal.\", \"Brad came to dinner with us.\", \"He loves fish tacos.\",\"In the end, we all felt like we ate too much.\",\"We all agreed; it was a magnificent evening.\"]\n\n# Tokenization of each document\n# tokenized_sent = []\n# for s in sentences:\n#     tokenized_sent.append(word_tokenize(s.lower()))\n# print(tokenized_sent)","metadata":{"id":"FDp9IMcK17KC","outputId":"f7c2dbcd-67bc-479b-f600-71088225993f","execution":{"iopub.status.busy":"2022-11-17T14:25:02.202009Z","iopub.execute_input":"2022-11-17T14:25:02.202743Z","iopub.status.idle":"2022-11-17T14:25:02.210944Z","shell.execute_reply.started":"2022-11-17T14:25:02.202706Z","shell.execute_reply":"2022-11-17T14:25:02.210034Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def cosine(u, v):\n    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))","metadata":{"id":"C1zmfAan2KT-","execution":{"iopub.status.busy":"2022-11-17T14:25:02.214078Z","iopub.execute_input":"2022-11-17T14:25:02.214450Z","iopub.status.idle":"2022-11-17T14:25:02.221634Z","shell.execute_reply.started":"2022-11-17T14:25:02.214405Z","shell.execute_reply":"2022-11-17T14:25:02.220551Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\n# import gensim\n# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n# tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n# tagged_data","metadata":{"id":"BDXhq1ac2Bq8","outputId":"5c56ea5f-a2f5-4293-cde3-b292892eb3c8","execution":{"iopub.status.busy":"2022-11-17T14:25:02.223138Z","iopub.execute_input":"2022-11-17T14:25:02.223637Z","iopub.status.idle":"2022-11-17T14:25:02.233158Z","shell.execute_reply.started":"2022-11-17T14:25:02.223599Z","shell.execute_reply":"2022-11-17T14:25:02.232263Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"## Train doc2vec model\n# model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n\n# '''\n# vector_size = Dimensionality of the feature vectors.\n# window = The maximum distance between the current and predicted word within a sentence.\n# min_count = Ignores all words with total frequency lower than this.\n# alpha = The initial learning rate.\n# '''\n\n## Print model vocabulary\n# model.wv.vocab","metadata":{"id":"1DMAZaNc2-hr","outputId":"ee73e5e4-c815-4376-a1fe-ace3686e0a75","execution":{"iopub.status.busy":"2022-11-17T14:25:02.236575Z","iopub.execute_input":"2022-11-17T14:25:02.236992Z","iopub.status.idle":"2022-11-17T14:25:02.245915Z","shell.execute_reply.started":"2022-11-17T14:25:02.236957Z","shell.execute_reply":"2022-11-17T14:25:02.245009Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n# test_doc_vector = model.infer_vector(test_doc)\n# model.docvecs.most_similar(positive = [test_doc_vector])\n\n# '''\n# positive = List of sentences that contribute positively.\n# '''","metadata":{"id":"VBFNNhj73EoO","outputId":"e369a662-db8f-48ee-d0be-610f44b98863","execution":{"iopub.status.busy":"2022-11-17T14:25:02.247359Z","iopub.execute_input":"2022-11-17T14:25:02.248049Z","iopub.status.idle":"2022-11-17T14:25:02.256892Z","shell.execute_reply.started":"2022-11-17T14:25:02.248013Z","shell.execute_reply":"2022-11-17T14:25:02.255864Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## SentenceBERT","metadata":{"id":"DCQ2T4zT5UON"}},{"cell_type":"code","source":"# !pip install sentence-transformers","metadata":{"id":"d5Kd1-u23KXF","outputId":"6b253cee-feee-4f25-ac1c-3d9ecec45b0f","execution":{"iopub.status.busy":"2022-11-17T14:25:02.259236Z","iopub.execute_input":"2022-11-17T14:25:02.259902Z","iopub.status.idle":"2022-11-17T14:25:02.266498Z","shell.execute_reply.started":"2022-11-17T14:25:02.259858Z","shell.execute_reply":"2022-11-17T14:25:02.265180Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\n# from sentence_transformers import SentenceTransformer\n# sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')","metadata":{"id":"3SFHJsxw5aiM","execution":{"iopub.status.busy":"2022-11-17T14:25:02.269478Z","iopub.execute_input":"2022-11-17T14:25:02.269962Z","iopub.status.idle":"2022-11-17T14:25:02.276097Z","shell.execute_reply.started":"2022-11-17T14:25:02.269937Z","shell.execute_reply":"2022-11-17T14:25:02.275087Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# sentence_embeddings = sbert_model.encode(sentences)\n\n# print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n# print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n# sentence_embeddings.shape","metadata":{"id":"6XHzXTfu5d16","outputId":"51bc4973-62ac-4db6-e00f-a40a6c1f6d08","execution":{"iopub.status.busy":"2022-11-17T14:25:02.277616Z","iopub.execute_input":"2022-11-17T14:25:02.278056Z","iopub.status.idle":"2022-11-17T14:25:02.285629Z","shell.execute_reply.started":"2022-11-17T14:25:02.278024Z","shell.execute_reply":"2022-11-17T14:25:02.284426Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# # keep the first 100 embeddings for each sentence in a separate list\n# embeddings = [sentence_embeddings[i][:100] for i in range(len(sentence_embeddings))]\n# embeddings[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.286768Z","iopub.execute_input":"2022-11-17T14:25:02.287025Z","iopub.status.idle":"2022-11-17T14:25:02.295762Z","shell.execute_reply.started":"2022-11-17T14:25:02.287002Z","shell.execute_reply":"2022-11-17T14:25:02.294740Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.297274Z","iopub.execute_input":"2022-11-17T14:25:02.297665Z","iopub.status.idle":"2022-11-17T14:25:02.310977Z","shell.execute_reply.started":"2022-11-17T14:25:02.297630Z","shell.execute_reply":"2022-11-17T14:25:02.310053Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Read csv file train.csv and store it in a variable called train_data\ntrain_data = pd.read_csv('../input/feedback-prize-effectiveness/train.csv')\ntrain_data.head()","metadata":{"id":"TXzfmwFK6cDs","execution":{"iopub.status.busy":"2022-11-17T14:25:02.346147Z","iopub.execute_input":"2022-11-17T14:25:02.346428Z","iopub.status.idle":"2022-11-17T14:25:02.485675Z","shell.execute_reply.started":"2022-11-17T14:25:02.346404Z","shell.execute_reply":"2022-11-17T14:25:02.484424Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"   discourse_id      essay_id  \\\n0  0013cc385424  007ACE74B050   \n1  9704a709b505  007ACE74B050   \n2  c22adee811b6  007ACE74B050   \n3  a10d361e54e4  007ACE74B050   \n4  db3e453ec4e2  007ACE74B050   \n\n                                      discourse_text discourse_type  \\\n0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n1  On my perspective, I think that the face is a ...       Position   \n2  I think that the face is a natural landform be...          Claim   \n3  If life was on Mars, we would know by now. The...       Evidence   \n4  People thought that the face was formed by ali...   Counterclaim   \n\n  discourse_effectiveness  \n0                Adequate  \n1                Adequate  \n2                Adequate  \n3                Adequate  \n4                Adequate  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>discourse_id</th>\n      <th>essay_id</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_effectiveness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0013cc385424</td>\n      <td>007ACE74B050</td>\n      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n      <td>Lead</td>\n      <td>Adequate</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9704a709b505</td>\n      <td>007ACE74B050</td>\n      <td>On my perspective, I think that the face is a ...</td>\n      <td>Position</td>\n      <td>Adequate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c22adee811b6</td>\n      <td>007ACE74B050</td>\n      <td>I think that the face is a natural landform be...</td>\n      <td>Claim</td>\n      <td>Adequate</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a10d361e54e4</td>\n      <td>007ACE74B050</td>\n      <td>If life was on Mars, we would know by now. The...</td>\n      <td>Evidence</td>\n      <td>Adequate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>db3e453ec4e2</td>\n      <td>007ACE74B050</td>\n      <td>People thought that the face was formed by ali...</td>\n      <td>Counterclaim</td>\n      <td>Adequate</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create a list of sentences from the train_data - train_data['discourse_text']\ntrain_sentences = train_data['discourse_text'].tolist()\n\ndiscourse_type = train_data['discourse_type'].tolist()\n# discourse_type","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.696175Z","iopub.execute_input":"2022-11-17T14:25:02.696555Z","iopub.status.idle":"2022-11-17T14:25:02.709768Z","shell.execute_reply.started":"2022-11-17T14:25:02.696522Z","shell.execute_reply":"2022-11-17T14:25:02.708756Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import csv\n\ndef readCSV(filename):\n    rows = []\n    with open(filename, 'r') as file:\n        csvreader = csv.reader(file)\n        header = next(csvreader)\n        for row in csvreader:\n            rows.append(row)\n    return rows","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.712078Z","iopub.execute_input":"2022-11-17T14:25:02.713189Z","iopub.status.idle":"2022-11-17T14:25:02.720612Z","shell.execute_reply.started":"2022-11-17T14:25:02.713150Z","shell.execute_reply":"2022-11-17T14:25:02.719239Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"discourses = []\ndis_types = []\ndis_effectiveness = []\nrows = readCSV(\"../input/feedback-prize-effectiveness/train.csv\")\nfor row in rows:\n    if len(row[2])<=510:\n        discourses.append(row[2])\n        dis_types.append(row[3])\n        dis_effectiveness.append(row[4])\n    while len(row[2])>510:\n        temp = row[2][:510]\n        row[2] = row[2][510:]\n        discourses.append(temp)\n        dis_types.append(row[3])\n        dis_effectiveness.append(row[4])\nprint(len(discourses),discourses[0])\nprint(len(dis_types))\nprint(len(dis_effectiveness))","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.722061Z","iopub.execute_input":"2022-11-17T14:25:02.723932Z","iopub.status.idle":"2022-11-17T14:25:02.927299Z","shell.execute_reply.started":"2022-11-17T14:25:02.723894Z","shell.execute_reply":"2022-11-17T14:25:02.926309Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"37762 Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. \n37762\n37762\n","output_type":"stream"}]},{"cell_type":"code","source":"typeDict = {'Lead':0,'Position':1,'Claim':2,'Counterclaim':3,'Rebuttal':4,'Evidence':5,'Concluding Statement':6}\neffectDict = {'Ineffective':0,'Adequate':1,'Effective':2}","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.928616Z","iopub.execute_input":"2022-11-17T14:25:02.929002Z","iopub.status.idle":"2022-11-17T14:25:02.934386Z","shell.execute_reply.started":"2022-11-17T14:25:02.928973Z","shell.execute_reply":"2022-11-17T14:25:02.933335Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# dis_types","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.937847Z","iopub.execute_input":"2022-11-17T14:25:02.938903Z","iopub.status.idle":"2022-11-17T14:25:02.946029Z","shell.execute_reply.started":"2022-11-17T14:25:02.938864Z","shell.execute_reply":"2022-11-17T14:25:02.944954Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# %pip install sent2vec","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.947524Z","iopub.execute_input":"2022-11-17T14:25:02.948103Z","iopub.status.idle":"2022-11-17T14:25:02.955227Z","shell.execute_reply.started":"2022-11-17T14:25:02.948065Z","shell.execute_reply":"2022-11-17T14:25:02.954031Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport gensim\nimport torch\nimport transformers as ppb\n# from sent2vec.splitter import *\n\nimport os\nimport re\nimport spacy\n\nROOT_DIR = os.getcwd() \nTEST_DIR = os.path.join(ROOT_DIR, 'test')\nDATA_DIR = os.path.join(TEST_DIR, 'dataset')\n\nROOT_LOCAL = '/Users/pedramataee/'\nROOT_LINUX = '~'\n\nWIKI_PATH = os.path.join(ROOT_LINUX, 'gensim-data/glove-wiki-gigaword-300') \nPRETRAINED_VECTORS_PATH_WIKI = os.path.join(WIKI_PATH, 'glove-wiki-gigaword-300.gz')\n\nFASTTEXT_NEWS_PATH = os.path.join(ROOT_LINUX, 'gensim-data/fasttext-wiki-news-subwords-300') \nPRETRAINED_VECTORS_PATH_FASTTEXT = os.path.join(FASTTEXT_NEWS_PATH, 'fasttext-wiki-news-subwords-300.gz')\n\n# Make sure to download \"en_core_web_sm\" package on your machine.\n# In case, you can run: \"python3 -m spacy download en_core_web_sm\"\nos.environ['LANGUAGE_MODEL_SPACY'] = \"en_core_web_sm\"\n\n\nclass Splitter:\n    def __init__(self):\n        self.words = []\n        self.sentences = []\n        try:\n            self.nlp = spacy.load(os.environ['LANGUAGE_MODEL_SPACY'])\n        except Exception as error:\n            print(f'{error}\\n\\n Install \"en_core_web_sm\" in your environment. '\n                  f'Try running: \"python3 -m spacy download en_core_web_sm\" \\n '\n                  f'or follow instrucions here: https://spacy.io/usage')\n\n    def sent2words(self, sentences, **kwargs):\n        add_stop_words = kwargs.get('add_stop_words', [])\n        remove_stop_words = kwargs.get('remove_stop_words', [])\n\n        for w in add_stop_words:\n            self.nlp.vocab[w].is_stop = True\n        for w in remove_stop_words:\n            self.nlp.vocab[w].is_stop = False\n\n        words = []\n        for sentence in sentences:\n            doc = self.nlp(sentence.lower())\n            words.append([token.lemma_ for token in doc if not token.is_punct | token.is_space | token.is_stop])\n\n        self.words = words\n\n    def text2sents(self, texts):\n        for text in texts:\n            doc = self.nlp(text)\n            span = doc[0:5]\n            sents = list(doc.sents)\n            self.sentences.extend([sent for sent in sents])\n\n    def text2words(self, texts):\n        doc = self.nlp(texts)\n        tokenized_texts = []\n        for w in doc:\n            is_clean = w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num\n            if is_clean:\n                tokenized_texts.append(w.lemma_)\n\n        self.words = tokenized_texts\n\n\ndef sentencizer_by_regex(texts):\n    alphabets = \"([A-Za-z])\"\n    prefixes = r\"(Mr|St|Mrs|Ms|Dr)[.]\"\n    suffixes = r\"(Inc|Ltd|Jr|Sr|Co|etc)\"\n    starters = r\"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n    acronyms = r\"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n    websites = r\"[.](com|net|org|io|gov)\"\n\n    text = \" \" + texts + \"  \"\n    text = text.replace(\"\\n\", \" \")\n    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n    text = re.sub(websites, \"<prd>\\\\1\", text)\n    if \"Ph.D\" in text:\n        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n    text = re.sub(r\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n    text = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n    text = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n    if \"”\" in text:\n        text = text.replace(\".”\", \"”.\")\n    if \"\\\"\" in text:\n        text = text.replace(\".\\\"\", \"\\\".\")\n    if \"!\" in text:\n        text = text.replace(\"!\\\"\", \"\\\"!\")\n    if \"?\" in text:\n        text = text.replace(\"?\\\"\", \"\\\"?\")\n    text = text.replace(\".\", \".<stop>\")\n    text = text.replace(\"?\", \"?<stop>\")\n    text = text.replace(\"!\", \"!<stop>\")\n    text = text.replace(\"<prd>\", \".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n\nclass Vectorizer:\n    \"\"\"\n    pretrained_weights: str, default='distilbert-base-uncased'\n\n        If the string does not include an extension .txt, .gz or .bin, then Bert vectorizer is loaded using the specified weights.\n        Example: pass 'distilbert-base-multilingual-cased' to load Bert base multilingual model.\n\n        To load word2vec vectorizer pass a valid path to the weights file (.txt, .gz or .bin).\n        Example: pass 'glove-wiki-gigaword-300.gz' to load the Wiki vectors (when saved in the same folder you are running the code).\n\n    \n    ensemble_method: str, default='average'\n\n        How word vectors are computed into sentece vectors.\n    \n    \"\"\"\n    def __init__(self, pretrained_weights = 'distilbert-base-uncased',\n                       ensemble_method = 'average'):\n        _, ext = os.path.splitext(pretrained_weights)\n        self.vectors = []\n        if not ext:\n            print(f'Initializing Bert {pretrained_weights}')\n            self.vectorizer = BertVectorizer(pretrained_weights=pretrained_weights)\n        else:\n            print(f'Initializing word2vec with vector path {pretrained_weights}')\n            self.vectorizer = GensimVectorizer(pretrained_weights=pretrained_weights, \n                                               ensemble_method=ensemble_method)\n\n    def run(self, sentences, remove_stop_words = ['not'], add_stop_words = []):\n        # SANITY CHECK\n        assert type(sentences) == list, 'A list must be passed!'\n        for sentence in sentences:\n            if type(sentence) != str:\n                raise TypeError(f'All items must be string type but {sentence} is type {type(sentence)}.')\n        # RUN\n        self.vectorizer._execute(sentences, remove_stop_words=remove_stop_words, add_stop_words=add_stop_words)\n        vectors = self.vectorizer.vectors\n        for idx in range(vectors.shape[0]):\n            self.vectors.append(vectors[idx])\n\n\nclass BaseVectorizer():\n    def __init__(self, **kwargs):\n        self.pretrained_weights = kwargs.get('pretrained_weights')\n        self.ensemble_method = kwargs.get('ensemble_method')\n        self.vectors = []\n    \n    def _load_model(self):\n        pass\n\n    def _execute(self):\n        pass\n\n\nclass BertVectorizer(BaseVectorizer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._load_model()\n    \n    def _load_model(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f'Vectorization done on {self.device}')\n        model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel,\n                                                            ppb.DistilBertTokenizer,\n                                                            self.pretrained_weights)\n        self.tokenizer = tokenizer_class.from_pretrained('../input/huggingface-bert/bert-base-uncased',local_files_only = True)\n        self.model = model_class.from_pretrained('../input/huggingface-bert/bert-base-uncased',local_files_only = True)\n    \n    def _execute(self, sentences, **kwargs):\n        model = self.model.to(self.device)\n        model.eval()\n        tokenized = list(map(lambda x: self.tokenizer.encode(x, add_special_tokens=True), sentences))\n        max_len = 0\n        for i in tokenized:\n            if len(i) > max_len:\n                max_len = len(i)\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        # Move inputs to same device as model\n        input_ids = torch.tensor(np.array(padded)).type(torch.LongTensor).to(self.device)\n        with torch.no_grad():\n            last_hidden_states = model(input_ids)\n        # Move vector results back to cpu if calculation was done on GPU\n        vectors = last_hidden_states[0][:, 0, :].cpu().numpy()\n        self.vectors = vectors\n\nclass GensimVectorizer(BaseVectorizer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._load_model()\n    \n    def _load_model(self):\n        _, file_extension = os.path.splitext(self.pretrained_weights)\n        # Checks if file extension is binary\n        if file_extension == '.bin':\n            self.model = gensim.models.KeyedVectors.load_word2vec_format(self.pretrained_weights, binary=True)\n        elif file_extension == '.txt' or '.gz':\n            self.model = gensim.models.KeyedVectors.load_word2vec_format(self.pretrained_weights)\n        else:\n            raise IOError(f'The file extension {file_extension} is not valid. Word2vec valid formats are \".txt\" and \".bin\".')\n    \n    def _execute(self, sentences, **kwargs):\n        splitter = Splitter()\n        splitter.sent2words(sentences, remove_stop_words=kwargs.get('remove_stop_words'), add_stop_words=kwargs.get('add_stop_words'))\n        words = splitter.words\n        vectors = []\n        for element in words:\n            temp = []\n            for w in element:\n                temp.append(self.model[w])\n            if self.ensemble_method == 'average':\n                element_vec = np.mean(temp, axis=0)\n                try:\n                    vectors = np.vstack([vectors, element_vec])\n                except:\n                    vectors = element_vec\n        self.vectors = vectors\n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:25:02.957238Z","iopub.execute_input":"2022-11-17T14:25:02.957615Z","iopub.status.idle":"2022-11-17T14:25:02.998197Z","shell.execute_reply.started":"2022-11-17T14:25:02.957580Z","shell.execute_reply":"2022-11-17T14:25:02.997338Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# from sent2vec.vectorizer import Vectorizer\nimport numpy as np\n\ndef getVectors(discourses, dis_types):\n    vectorizer = Vectorizer()\n    dis_vectors = []\n    for i in range(0,len(discourses),100):\n        vectorizer.run(discourses[i:min(i+100,len(discourses))], remove_stop_words=[], add_stop_words=[])\n        temp = vectorizer.vectors\n        for t in range(i,min(i+100,len(discourses))):\n            dis_vectors.append(temp[t][:100])\n    temp = dis_vectors\n    dis_vectors=[]\n    for i in range(len(temp)):\n        if (i==0):\n            dis_vectors.append([*[0 for j in range(100)],*temp[i],*temp[i+1]])\n        elif (i==len(temp)-1):\n            dis_vectors.append([*temp[i-1],*temp[i],*[0 for j in range(100)]])\n        else:\n            dis_vectors.append([*temp[i-1],*temp[i],*temp[i+1]])\n    dis_vectors = np.array(dis_vectors)      \n    # concatenate the type[dis_types[i]] and dis_vectors\n#     dis_vectors = np.concatenate((np.array([typeDict[dis_types[i]] for i in range(len(dis_types))]).reshape(-1,1),dis_vectors),axis=1)\n        \n\n#     print(len(dis_vectors))\n#     print(dis_vectors[0])\n    # return dis_vectors\n\n    output1 = []\n    output2 = []\n    for i in range(0,len(dis_types)):\n#         temp = [0 for i in range(0,len(typeDict))]\n#         temp[typeDict[dis_types[i]]]=1\n#         output1.append(temp)\n        output1.append(typeDict[dis_types[i]])\n    output1 = np.array(output1)\n#     print(len(output1))\n#     print(output1[0])\n    # make the output as  the one hot encoding of the effectDict[dis_effectiveness[i]]\n    for i in range(len(dis_effectiveness)):\n#         temp = [0,0,0]\n#         temp[effectDict[dis_effectiveness[i]]] = 1\n#         output2.append(temp)\n        output2.append(effectDict[dis_effectiveness[i]])\n    output2 = np.array(output2)\n#     print(len(output2))\n#     print(output2[0])\n    return dis_vectors,output1,output2\n\n# train_input, train_output = getVectors(discourses, dis_types)\n# getVectors(discourses, dis_types)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:51:53.878526Z","iopub.execute_input":"2022-11-17T14:51:53.878981Z","iopub.status.idle":"2022-11-17T14:51:53.898253Z","shell.execute_reply.started":"2022-11-17T14:51:53.878941Z","shell.execute_reply":"2022-11-17T14:51:53.897132Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"train_input, train_output_1, train_output_2 = getVectors(discourses, dis_types)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:52:17.300283Z","iopub.execute_input":"2022-11-17T14:52:17.300694Z","iopub.status.idle":"2022-11-17T14:57:11.826857Z","shell.execute_reply.started":"2022-11-17T14:52:17.300644Z","shell.execute_reply":"2022-11-17T14:57:11.825780Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BertTokenizer'. \nThe class this function is called from is 'DistilBertTokenizer'.\nYou are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Initializing Bert distilbert-base-uncased\nVectorization done on cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-bert/bert-base-uncased were not used when initializing DistilBertModel: ['bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'cls.predictions.decoder.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'cls.predictions.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertModel were not initialized from the model checkpoint at ../input/huggingface-bert/bert-base-uncased and are newly initialized: ['transformer.layer.7.ffn.lin1.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin2.bias', 'embeddings.LayerNorm.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.3.attention.out_lin.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.8.ffn.lin1.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.11.attention.k_lin.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_input.shape\n# dum = [*[0 for j in range(100)],*train_input[0],*train_input[1]]\n# len(dum)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:00:21.017768Z","iopub.execute_input":"2022-11-17T15:00:21.018144Z","iopub.status.idle":"2022-11-17T15:00:21.028593Z","shell.execute_reply.started":"2022-11-17T15:00:21.018112Z","shell.execute_reply":"2022-11-17T15:00:21.027554Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"(37762, 300)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\ndef generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n  \"\"\"\n  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n\n  Some examples of different formats of class_series and their outputs are:\n    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n\n  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n  of appareance of the label when the dataset was processed. \n  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n\n  Author: Angel Igareta (angel@igareta.com)\n  \"\"\"\n  if multi_class:\n    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n    if one_hot_encoded:\n      class_series = np.argmax(class_series, axis=1)\n  \n    # Compute class weights with sklearn method\n    class_labels = np.unique(class_series)\n    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n    return dict(zip(class_labels, class_weights))\n  else:\n    # It is neccessary that the multi-label values are one-hot encoded\n    mlb = None\n    if not one_hot_encoded:\n      mlb = MultiLabelBinarizer()\n      class_series = mlb.fit_transform(class_series)\n\n    n_samples = len(class_series)\n    n_classes = len(class_series[0])\n\n    # Count each class frequency\n    class_count = [0] * n_classes\n    for classes in class_series:\n        for index in range(n_classes):\n            if classes[index] != 0:\n                class_count[index] += 1\n    \n    # Compute class weights using balanced method\n    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n    return dict(zip(class_labels, class_weights))","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:00:24.819405Z","iopub.execute_input":"2022-11-17T15:00:24.820626Z","iopub.status.idle":"2022-11-17T15:00:24.830795Z","shell.execute_reply.started":"2022-11-17T15:00:24.820585Z","shell.execute_reply":"2022-11-17T15:00:24.829923Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class_weights1 = generate_class_weights(train_output_1)\nclass_weights2 = generate_class_weights(train_output_2)\nclass_weights2","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:00:30.722235Z","iopub.execute_input":"2022-11-17T15:00:30.725659Z","iopub.status.idle":"2022-11-17T15:00:30.766244Z","shell.execute_reply.started":"2022-11-17T15:00:30.725596Z","shell.execute_reply":"2022-11-17T15:00:30.765137Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{0: 1.8367624884478817, 1: 0.5972354020370721, 2: 1.2801111902098377}"},"metadata":{}}]},{"cell_type":"code","source":"def weighted_categorical_crossentropy(class_weight):\n    def loss(y_obs, y_pred):\n        y_obs = tf.dtypes.cast(y_obs, tf.int32)\n        hothot = tf.one_hot(tf.reshape(y_obs,[-1]),depth=len(class_weight))\n#         weight = tf.math.multiply(class_weight, hothot)\n#         weight = tf.reduce_sum(weight, axis=-1)\n        losses = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n            labels=y_obs, logits=y_pred\n        )\n        return losses\n    return loss","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-17T15:13:36.461125Z","iopub.execute_input":"2022-11-17T15:13:36.461527Z","iopub.status.idle":"2022-11-17T15:13:36.468201Z","shell.execute_reply.started":"2022-11-17T15:13:36.461496Z","shell.execute_reply":"2022-11-17T15:13:36.467026Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"loss1 = weighted_categorical_crossentropy(list(class_weights1.values()))\nloss2 = weighted_categorical_crossentropy(list(class_weights2.values()))","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:13:39.319256Z","iopub.execute_input":"2022-11-17T15:13:39.320186Z","iopub.status.idle":"2022-11-17T15:13:39.326857Z","shell.execute_reply.started":"2022-11-17T15:13:39.320138Z","shell.execute_reply":"2022-11-17T15:13:39.325908Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"train_output_1.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:00:41.156057Z","iopub.execute_input":"2022-11-17T15:00:41.156477Z","iopub.status.idle":"2022-11-17T15:00:41.163213Z","shell.execute_reply.started":"2022-11-17T15:00:41.156443Z","shell.execute_reply":"2022-11-17T15:00:41.162108Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"(37762,)"},"metadata":{}}]},{"cell_type":"code","source":"# design a tensorflow keras classfication model\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom math import ceil\n\n# train_input = np.array(train_input)\n# train_output = np.array(train_output)\n\n# print(train_input.shape)\n# print(train_output.shape)\n\n# model = keras.Sequential([\n#     layers.Dense(128, activation='relu', input_shape=[101]),\n#     layers.Dense(64, activation='relu'),\n#     layers.Dense(3, activation='softmax')\n# ])\n\n\n\ndef build_model():\n    input_layer = layers.Input(shape=(300,))\n    X = layers.Dense(128, activation='relu')(input_layer)\n#     X = layers.Dropout(0.5)(X)\n    X = layers.Dense(64, activation='relu')(X)\n#     X = layers.Dropout(0.5)(X)\n    output_layer_1 = layers.Dense(7,name=\"output_1\")(X)\n    output_layer_2 = layers.Dense(3,name=\"output_2\")(output_layer_1)\n    \n    model = tf.keras.Model(inputs=[input_layer], outputs=[output_layer_1,output_layer_2])\n    return model\nmodel = build_model()\n    \n\n# model.compile(\n#     optimizer='adam',\n#     loss='categorical_crossentropy',\n#     metrics=['accuracy'])\n\nmodel.compile(\n    optimizer='adam',\n    loss={\"output_1\":loss1, \"output_2\": loss2},\n    metrics={\"output_1\":loss1,\"output_2\":loss2}\n)\ntrain_output = [train_output_1, train_output_2]\n# X_train, X_test, y_train, y_test = train_test_split(train_input, train_output, test_size=0.2)\nX_train = train_input[:ceil(.8*len(train_input))]\ny_train = [train_output_1[:ceil(.8*len(train_output_1))],train_output_2[:ceil(.8*len(train_output_2))]]\nX_test = train_input[ceil(.8*len(train_input)):]\ny_test = [train_output_1[ceil(.8*len(train_output_1)):],train_output_2[ceil(.8*len(train_output_2)):]]\nmodel.fit(X_train, y_train, epochs=20, batch_size=15)\nmodel.evaluate(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:28:11.907204Z","iopub.execute_input":"2022-11-17T15:28:11.907668Z","iopub.status.idle":"2022-11-17T15:30:34.685779Z","shell.execute_reply.started":"2022-11-17T15:28:11.907631Z","shell.execute_reply":"2022-11-17T15:30:34.684086Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.4169 - output_1_loss: 1.4507 - output_2_loss: 0.9663\nEpoch 2/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.2241 - output_1_loss: 1.2818 - output_2_loss: 0.9423\nEpoch 3/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.1620 - output_1_loss: 1.2444 - output_2_loss: 0.9176\nEpoch 4/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.1322 - output_1_loss: 1.2266 - output_2_loss: 0.9056\nEpoch 5/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 2.1096 - output_1_loss: 1.2114 - output_2_loss: 0.8982\nEpoch 6/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0933 - output_1_loss: 1.2028 - output_2_loss: 0.8905\nEpoch 7/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 2.0784 - output_1_loss: 1.1918 - output_2_loss: 0.8866\nEpoch 8/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0638 - output_1_loss: 1.1828 - output_2_loss: 0.8809\nEpoch 9/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0493 - output_1_loss: 1.1736 - output_2_loss: 0.8756\nEpoch 10/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0409 - output_1_loss: 1.1693 - output_2_loss: 0.8716\nEpoch 11/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 2.0362 - output_1_loss: 1.1639 - output_2_loss: 0.8723\nEpoch 12/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0232 - output_1_loss: 1.1564 - output_2_loss: 0.8667\nEpoch 13/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 2.0213 - output_1_loss: 1.1531 - output_2_loss: 0.8681\nEpoch 14/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0132 - output_1_loss: 1.1472 - output_2_loss: 0.8660\nEpoch 15/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 2.0075 - output_1_loss: 1.1446 - output_2_loss: 0.8629\nEpoch 16/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 2.0106 - output_1_loss: 1.1471 - output_2_loss: 0.8636\nEpoch 17/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 2.0001 - output_1_loss: 1.1410 - output_2_loss: 0.8591\nEpoch 18/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 1.9993 - output_1_loss: 1.1386 - output_2_loss: 0.8607\nEpoch 19/20\n2014/2014 [==============================] - 5s 3ms/step - loss: 1.9919 - output_1_loss: 1.1347 - output_2_loss: 0.8572\nEpoch 20/20\n2014/2014 [==============================] - 6s 3ms/step - loss: 1.9891 - output_1_loss: 1.1325 - output_2_loss: 0.8566\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3979093576.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_output_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_output_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'mode' is not defined"],"ename":"NameError","evalue":"name 'mode' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model.evaluate(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:30:48.001206Z","iopub.execute_input":"2022-11-17T15:30:48.001957Z","iopub.status.idle":"2022-11-17T15:30:48.924300Z","shell.execute_reply.started":"2022-11-17T15:30:48.001918Z","shell.execute_reply":"2022-11-17T15:30:48.923388Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"236/236 [==============================] - 1s 2ms/step - loss: 2.0933 - output_1_loss: 1.1028 - output_2_loss: 0.9906\n","output_type":"stream"},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"[2.0933430194854736,\n 1.1027781963348389,\n 0.9905652403831482,\n 1.1027781963348389,\n 0.9905652403831482]"},"metadata":{}}]},{"cell_type":"code","source":"# create a submissions.csv file\n# '''\n# discourse_id,Ineffective,Adequate,Effective\n# a261b6e14276,0.2,0.6,0.4\n# 5a88900e7dc1,3.0,6.0,1.0\n# 9790d835736b,1.0,2.0,3.0\n# 75ce6d68b67b,0.33,0.34,0.33\n# 93578d946723,0.01,0.24,0.47\n# 2e214524dbe3,0.2,0.6,0.4\n# 84812fc2ab9f,3.0,6.0,1.0\n# c668ff840720,1.0,2.0,3.0\n# 739a6d00f44a,0.33,0.34,0.33\n# bcfae2c9a244,0.01,0.24,0.47\n# '''\n\ntest_data = pd.read_csv('../input/feedback-prize-effectiveness/test.csv')\ntest_data.head()\n\ntest_sentences = test_data['discourse_text'].tolist()\n# test_sentences[0]\n\ntest_dis_types  = test_data['discourse_type'].tolist()\n\ntest_input,_,_ = getVectors(test_sentences,test_dis_types)\ntest_input = np.array(test_input)\n\ntest_output = model.predict(test_input)\n# test_output = np.argmax(test_output, axis=1)\n# print(test_output)\n\nimport csv\n\nwith open('submission.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"])\n    for i in range(len(test_output[1])):\n#         temp = [0,0,0]\n#         temp[test_output[i]] = 1\n        writer.writerow([test_data['discourse_id'][i],test_output[1][i][0],test_output[1][i][1],test_output[1][i][2]])","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:16:39.732927Z","iopub.execute_input":"2022-11-17T15:16:39.733333Z","iopub.status.idle":"2022-11-17T15:16:42.144485Z","shell.execute_reply.started":"2022-11-17T15:16:39.733302Z","shell.execute_reply":"2022-11-17T15:16:42.143461Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BertTokenizer'. \nThe class this function is called from is 'DistilBertTokenizer'.\nYou are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Initializing Bert distilbert-base-uncased\nVectorization done on cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-bert/bert-base-uncased were not used when initializing DistilBertModel: ['bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'cls.predictions.decoder.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'cls.predictions.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertModel were not initialized from the model checkpoint at ../input/huggingface-bert/bert-base-uncased and are newly initialized: ['transformer.layer.7.ffn.lin1.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin2.bias', 'embeddings.LayerNorm.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.3.attention.out_lin.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.8.ffn.lin1.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.11.attention.k_lin.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_output[1][0][0]","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:29:57.620018Z","iopub.status.idle":"2022-11-17T14:29:57.620387Z","shell.execute_reply.started":"2022-11-17T14:29:57.620199Z","shell.execute_reply":"2022-11-17T14:29:57.620216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get accuracy of the model\n# from sklearn.metrics import accuracy_score\n# print(accuracy_score(train_output, model.predict(train_input)))","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:29:57.627289Z","iopub.status.idle":"2022-11-17T14:29:57.628085Z","shell.execute_reply.started":"2022-11-17T14:29:57.627839Z","shell.execute_reply":"2022-11-17T14:29:57.627863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# import spacy","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:29:57.629446Z","iopub.status.idle":"2022-11-17T14:29:57.630300Z","shell.execute_reply.started":"2022-11-17T14:29:57.630014Z","shell.execute_reply":"2022-11-17T14:29:57.630042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define losses according to respective class weights\n#build model with two output layers\n#redifine get vectors, it will now return two output matrices\n#compil model\n#train","metadata":{"execution":{"iopub.status.busy":"2022-11-17T14:29:57.632079Z","iopub.status.idle":"2022-11-17T14:29:57.632869Z","shell.execute_reply.started":"2022-11-17T14:29:57.632578Z","shell.execute_reply":"2022-11-17T14:29:57.632601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}